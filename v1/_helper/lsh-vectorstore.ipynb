{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eade2909-168e-4316-90bf-6ec88bacb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable, List, Tuple, Dict, Optional, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d0dd6429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    import datetime\n",
    "    return datetime.datetime.now(datetime.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80ea33",
   "metadata": {},
   "source": [
    "# LSH Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc36bbc-a40f-4a27-9ba2-0641b897c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class LSHEmbedding:\n",
    "    \"\"\"Output of LSH embedder.\"\"\"\n",
    "    signature: List[int]          # MinHash signature (length = num_hashes)\n",
    "    band_keys: List[str]          # LSH bucket keys (length = num_bands)\n",
    "    shingle_count: int            # number of unique shingles used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d79a4a8-81ca-4316-b7a2-ab78dfb9e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHMinHashEmbedder:\n",
    "    \"\"\"\n",
    "    MinHash + LSH banding embedder for near-duplicate detection / candidate generation.\n",
    "\n",
    "    - Partial match support: character shingles (n-grams).\n",
    "    - Vocabulary-free: uses hashing, no growing vocab table.\n",
    "    - Persistent-friendly: `band_keys` are stable strings you can store (e.g., in Qdrant payload).\n",
    "\n",
    "    Typical usage:\n",
    "        emb = LSHMinHashEmbedder(shingle_size=5, num_hashes=128, bands=32, seed=42)\n",
    "        out = emb.embed(\"raw log line ...\")\n",
    "        # candidate retrieval: find items that share any band_key\n",
    "        # verification: recompute exact Jaccard of shingles (or other metric) vs candidates\n",
    "    \"\"\"\n",
    "\n",
    "    # --- regexes for common log normalization ---\n",
    "    _RE_UUID = re.compile(r\"\\b[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\\b\", re.I)\n",
    "    _RE_HEX  = re.compile(r\"\\b0x[0-9a-f]+\\b\", re.I)\n",
    "    _RE_IPv4 = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "    _RE_NUM  = re.compile(r\"\\b\\d+\\b\")\n",
    "    _RE_TS1  = re.compile(r\"\\b\\d{4}-\\d{2}-\\d{2}[T ]\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?Z?\\b\", re.I)  # ISO-ish\n",
    "    _RE_TS2  = re.compile(r\"\\b\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?\\b\")  # time only\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        shingle_size: int = 5,\n",
    "        num_hashes: int = 128,\n",
    "        bands: int = 32,\n",
    "        seed: int = 42,\n",
    "        normalize: bool = True,\n",
    "        lowercase: bool = True,\n",
    "        collapse_whitespace: bool = True,\n",
    "        stop_short_lines: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shingle_size: character n-gram size (5–7 is typical for logs)\n",
    "            num_hashes: MinHash signature length (64–256 typical)\n",
    "            bands: number of LSH bands. Must divide num_hashes exactly.\n",
    "            seed: controls determinism of the hash family\n",
    "            normalize: apply log normalization (uuid/ip/nums/timestamps masking)\n",
    "            lowercase: lowercase before shingling\n",
    "            collapse_whitespace: replace consecutive whitespace with single space\n",
    "            stop_short_lines: if >0 and normalized text shorter than this, returns empty shingles/signature behavior\n",
    "        \"\"\"\n",
    "        if shingle_size <= 0:\n",
    "            raise ValueError(\"shingle_size must be > 0\")\n",
    "        if num_hashes <= 0:\n",
    "            raise ValueError(\"num_hashes must be > 0\")\n",
    "        if bands <= 0 or (num_hashes % bands != 0):\n",
    "            raise ValueError(\"bands must be > 0 and must divide num_hashes exactly\")\n",
    "\n",
    "        self.shingle_size = shingle_size\n",
    "        self.num_hashes = num_hashes\n",
    "        self.bands = bands\n",
    "        self.rows_per_band = num_hashes // bands\n",
    "\n",
    "        self.seed = seed\n",
    "        self.normalize_enabled = normalize\n",
    "        self.lowercase = lowercase\n",
    "        self.collapse_whitespace = collapse_whitespace\n",
    "        self.stop_short_lines = stop_short_lines\n",
    "\n",
    "        # A large prime > 2^32 for modular hashing\n",
    "        self._prime = 4294967311  # near 2^32, prime\n",
    "        self._max_hash = (1 << 32) - 1\n",
    "\n",
    "        # Pre-generate hash function parameters (a_i, b_i)\n",
    "        # h_i(x) = (a_i * x + b_i) mod prime\n",
    "        self._a, self._b = self._make_hash_params(num_hashes, seed)\n",
    "\n",
    "    # ------------------------- Public API -------------------------\n",
    "\n",
    "    def embed(self, text: str) -> LSHEmbedding:\n",
    "        \"\"\"Compute MinHash signature + LSH band keys for a single log line.\"\"\"\n",
    "        norm = self._preprocess(text)\n",
    "        if self.stop_short_lines and len(norm) < self.stop_short_lines:\n",
    "            sig = [self._max_hash] * self.num_hashes\n",
    "            bands = self._signature_to_band_keys(sig)\n",
    "            return LSHEmbedding(signature=sig, band_keys=bands, shingle_count=0)\n",
    "\n",
    "        shingles = self._shingle(norm)\n",
    "        sig = self._minhash_signature(shingles)\n",
    "        bands = self._signature_to_band_keys(sig)\n",
    "        return LSHEmbedding(signature=sig, band_keys=bands, shingle_count=len(shingles))\n",
    "\n",
    "    def shingles(self, text: str) -> Set[int]:\n",
    "        \"\"\"Return hashed shingles (useful for exact Jaccard verification).\"\"\"\n",
    "        return self._shingle(self._preprocess(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard(shingles_a: Set[int], shingles_b: Set[int]) -> float:\n",
    "        \"\"\"Exact Jaccard similarity between two shingle sets.\"\"\"\n",
    "        if not shingles_a and not shingles_b:\n",
    "            return 1.0\n",
    "        if not shingles_a or not shingles_b:\n",
    "            return 0.0\n",
    "        inter = len(shingles_a & shingles_b)\n",
    "        union = len(shingles_a | shingles_b)\n",
    "        return inter / union\n",
    "\n",
    "    # ------------------------- Preprocess -------------------------\n",
    "\n",
    "    def _preprocess(self, text: str) -> str:\n",
    "        s = text\n",
    "        if self.lowercase:\n",
    "            s = s.lower()\n",
    "\n",
    "        if self.normalize_enabled:\n",
    "            s = self._RE_UUID.sub(\"<uuid>\", s)\n",
    "            s = self._RE_IPv4.sub(\"<ip>\", s)\n",
    "            s = self._RE_TS1.sub(\"<ts>\", s)\n",
    "            s = self._RE_TS2.sub(\"<time>\", s)\n",
    "            s = self._RE_HEX.sub(\"<hex>\", s)\n",
    "            s = self._RE_NUM.sub(\"<num>\", s)\n",
    "\n",
    "        if self.collapse_whitespace:\n",
    "            s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    # ------------------------- Shingling -------------------------\n",
    "\n",
    "    def _shingle(self, s: str) -> Set[int]:\n",
    "        \"\"\"\n",
    "        Character n-gram shingling -> set of 32-bit ints (stable).\n",
    "        Using a stable hash (blake2b) for shingles to avoid Python hash randomization.\n",
    "        \"\"\"\n",
    "        n = self.shingle_size\n",
    "        if len(s) < n:\n",
    "            return set()\n",
    "\n",
    "        out: Set[int] = set()\n",
    "        # sliding window\n",
    "        for i in range(0, len(s) - n + 1):\n",
    "            gram = s[i : i + n]\n",
    "            out.add(self._stable_u32(gram))\n",
    "        return out\n",
    "\n",
    "    # ------------------------- MinHash -------------------------\n",
    "\n",
    "    def _minhash_signature(self, shingles: Set[int]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Compute MinHash signature over hashed shingles.\n",
    "        If shingles empty: return max_hash vector so that it won't spuriously match.\n",
    "        \"\"\"\n",
    "        if not shingles:\n",
    "            return [self._max_hash] * self.num_hashes\n",
    "\n",
    "        sig = [self._max_hash] * self.num_hashes\n",
    "        p = self._prime\n",
    "\n",
    "        # For each shingle x, update all hash functions:\n",
    "        # sig[i] = min(sig[i], (a[i]*x + b[i]) % p)\n",
    "        # (This is O(num_hashes * num_shingles). For very long lines, consider sampling shingles.)\n",
    "        for x in shingles:\n",
    "            for i in range(self.num_hashes):\n",
    "                hx = (self._a[i] * x + self._b[i]) % p\n",
    "                if hx < sig[i]:\n",
    "                    sig[i] = hx\n",
    "\n",
    "        # Convert modulo prime values into 32-bit range for compactness\n",
    "        # (still deterministic; helps if you want to pack)\n",
    "        return [v & self._max_hash for v in sig]\n",
    "\n",
    "    def _signature_to_band_keys(self, sig: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convert signature to LSH band keys.\n",
    "        Band key is a stable string: \"lsh:{band_index}:{hex_digest}\".\n",
    "        \"\"\"\n",
    "        keys: List[str] = []\n",
    "        r = self.rows_per_band\n",
    "        for b in range(self.bands):\n",
    "            chunk = sig[b * r : (b + 1) * r]\n",
    "            # Stable digest of the chunk\n",
    "            digest = hashlib.blake2b(\n",
    "                (\",\".join(map(str, chunk))).encode(\"utf-8\"),\n",
    "                digest_size=8  # 64-bit digest -> short key\n",
    "            ).hexdigest()\n",
    "            keys.append(f\"lsh:{b}:{digest}\")\n",
    "        return keys\n",
    "\n",
    "    # ------------------------- Hash utilities -------------------------\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_hash_params(k: int, seed: int) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Deterministically generate (a_i, b_i) pairs for k hash functions.\n",
    "        We derive them from blake2b(seed||i) so results are stable across runs.\n",
    "        \"\"\"\n",
    "        a: List[int] = []\n",
    "        b: List[int] = []\n",
    "        for i in range(k):\n",
    "            h = hashlib.blake2b(f\"{seed}:{i}\".encode(\"utf-8\"), digest_size=16).digest()\n",
    "            # 64-bit a and b, then clamp into [1, prime-1] / [0, prime-1]\n",
    "            ai = int.from_bytes(h[:8], \"little\") | 1  # make it odd / non-zero-ish\n",
    "            bi = int.from_bytes(h[8:], \"little\")\n",
    "            a.append(ai)\n",
    "            b.append(bi)\n",
    "        return a, b\n",
    "\n",
    "    @staticmethod\n",
    "    def _stable_u32(s: str) -> int:\n",
    "        \"\"\"Stable 32-bit unsigned hash for a string.\"\"\"\n",
    "        # blake2b is fast and stable; digest_size=4 gives 32-bit\n",
    "        return int.from_bytes(hashlib.blake2b(s.encode(\"utf-8\"), digest_size=4).digest(), \"little\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad44620-7056-497d-9d16-70fb5c6a9fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A vs B band overlap: 11 out of 32\n",
      "A vs C band overlap: 1 out of 32\n",
      "A vs B jaccard: 0.8181818181818182\n",
      "A vs C jaccard: 0.3148148148148148\n"
     ]
    }
   ],
   "source": [
    "emb = LSHMinHashEmbedder(shingle_size=5, num_hashes=128, bands=32, seed=123)\n",
    "\n",
    "a = \"ERROR 2026-01-13T12:00:01Z user_id=123e4567-e89b-12d3-a456-426614174000 request took 153ms\"\n",
    "b = \"ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\"\n",
    "c = \"INFO 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user\"\n",
    "\n",
    "ea = emb.embed(a)\n",
    "eb = emb.embed(b)\n",
    "ec = emb.embed(c)\n",
    "\n",
    "# Candidate check: share any band?\n",
    "overlap_ab = set(ea.band_keys) & set(eb.band_keys)\n",
    "overlap_ac = set(ea.band_keys) & set(ec.band_keys)\n",
    "print(\"A vs B band overlap:\", len(overlap_ab), \"out of\", emb.bands)\n",
    "print(\"A vs C band overlap:\", len(overlap_ac), \"out of\", emb.bands)\n",
    "\n",
    "# Verify exact Jaccard on shingles\n",
    "ja = emb.shingles(a)\n",
    "jb = emb.shingles(b)\n",
    "jc = emb.shingles(c)\n",
    "print(\"A vs B jaccard:\", LSHMinHashEmbedder.jaccard(ja, jb))\n",
    "print(\"A vs C jaccard:\", LSHMinHashEmbedder.jaccard(ja, jc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47039a67-9110-47e0-b84f-7f2332447c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lsh:0:5b12bfbeb0ade072',\n",
       " 'lsh:1:e788d2732f29c5f9',\n",
       " 'lsh:2:ea56b5865e440b9d',\n",
       " 'lsh:3:a7f5210ee8333ff9',\n",
       " 'lsh:4:61e6122b314304a9',\n",
       " 'lsh:5:a24c3068b732600c',\n",
       " 'lsh:6:ddc276ec409d0015',\n",
       " 'lsh:7:f723b4ded7fd6af3',\n",
       " 'lsh:8:4aa4d7d63d232caf',\n",
       " 'lsh:9:b11b60769cc85d96',\n",
       " 'lsh:10:d65429182bc472c8',\n",
       " 'lsh:11:71328dd07180a8aa',\n",
       " 'lsh:12:fa688b12e812ff2b',\n",
       " 'lsh:13:f9240a08744e9750',\n",
       " 'lsh:14:c4b912d14465d9fd',\n",
       " 'lsh:15:d37011a6c7a52734',\n",
       " 'lsh:16:a40884bec6641e62',\n",
       " 'lsh:17:2f9e86428459eeb3',\n",
       " 'lsh:18:c9626c2fcdefd5b9',\n",
       " 'lsh:19:46f0ed02f7833081',\n",
       " 'lsh:20:61c76801151e84db',\n",
       " 'lsh:21:6ec7178c726b8bbc',\n",
       " 'lsh:22:8d01b1c5f517a04b',\n",
       " 'lsh:23:b3bcb51093d30cdd',\n",
       " 'lsh:24:0b3833e4bd553639',\n",
       " 'lsh:25:e4f908130211eacf',\n",
       " 'lsh:26:4ccb02a86ccd5642',\n",
       " 'lsh:27:7acefe4a27a71447',\n",
       " 'lsh:28:314c795d670b9caf',\n",
       " 'lsh:29:89e15f43c55a6476',\n",
       " 'lsh:30:f086277b30e57f5e',\n",
       " 'lsh:31:e6c02dcfbd6016a7']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea.band_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87b2d3",
   "metadata": {},
   "source": [
    "# Qdrant Adapter Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "863e4f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from qdrant_client.http import models\n",
    "\n",
    "\n",
    "Spec = Dict[str, Any]\n",
    "\n",
    "\n",
    "def _as_list(x: Any) -> List[Any]:\n",
    "    if x is None:\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def _parse_range_value(value: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - {\"gte\": ..., \"lte\": ...}\n",
    "      - [\"start\", \"end\"] / (\"start\", \"end\")\n",
    "      - \"start, end\"  (string)\n",
    "    Returns dict with keys: gte, gt, lte, lt (subset)\n",
    "    \"\"\"\n",
    "    if isinstance(value, dict):\n",
    "        return value\n",
    "\n",
    "    if isinstance(value, (list, tuple)) and len(value) == 2:\n",
    "        return {\"gte\": value[0], \"lte\": value[1]}\n",
    "\n",
    "    if isinstance(value, str) and \",\" in value:\n",
    "        a, b = [p.strip() for p in value.split(\",\", 1)]\n",
    "        return {\"gte\": a, \"lte\": b}\n",
    "\n",
    "    raise ValueError(f\"Unsupported range value format: {value!r}\")\n",
    "\n",
    "\n",
    "def _field_condition_from_atomic(spec: Spec) -> Union[models.FieldCondition, models.HasIdCondition]:\n",
    "    \"\"\"\n",
    "    Build a single Qdrant condition from one atomic spec.\n",
    "    \"\"\"\n",
    "    key = spec.get(\"key\")\n",
    "    dtype = (spec.get(\"dtype\") or \"string\").lower()\n",
    "    op = (spec.get(\"op\") or \"equals\").lower()\n",
    "    value = spec.get(\"value\", None)\n",
    "\n",
    "    if not key and op != \"has_id\":\n",
    "        raise ValueError(f\"Missing 'key' in spec: {spec}\")\n",
    "\n",
    "    # Special: filter by point ids (Qdrant IDs), not payload field\n",
    "    if op == \"has_id\":\n",
    "        ids = _as_list(value)\n",
    "        return models.HasIdCondition(has_id=ids)\n",
    "\n",
    "    # Common string exact match ops (keyword-like fields)\n",
    "    if op in {\"equals\", \"eq\"}:\n",
    "        return models.FieldCondition(key=key, match=models.MatchValue(value=value))\n",
    "\n",
    "    if op in {\"in\"}:\n",
    "        # value: list\n",
    "        return models.FieldCondition(key=key, match=models.MatchAny(any=_as_list(value)))\n",
    "\n",
    "    if op in {\"not_in\"}:\n",
    "        return models.FieldCondition(key=key, match=models.MatchExcept(except_=_as_list(value)))\n",
    "\n",
    "    # Text search (requires text index for best performance; still works otherwise)\n",
    "    if op in {\"contains\", \"text\"}:\n",
    "        # token-based match; for substring-ish search use text index\n",
    "        return models.FieldCondition(key=key, match=models.MatchText(text=str(value)))\n",
    "\n",
    "    if op in {\"phrase\"}:\n",
    "        return models.FieldCondition(key=key, match=models.MatchPhrase(phrase=str(value)))\n",
    "\n",
    "    if op in {\"prefix\"}:\n",
    "        # Qdrant supports MatchText; prefix semantics depend on tokenizer/index config.\n",
    "        # Many log use-cases are better handled by storing normalized keyword field(s).\n",
    "        return models.FieldCondition(key=key, match=models.MatchText(text=str(value)))\n",
    "\n",
    "    # Existence / null / empty checks\n",
    "    if op in {\"exists\", \"has_field\"}:\n",
    "        # \"is_null=False\" matches when field is present & not null\n",
    "        return models.FieldCondition(key=key, is_null=False)\n",
    "\n",
    "    if op in {\"is_null\"}:\n",
    "        return models.FieldCondition(key=key, is_null=True)\n",
    "\n",
    "    if op in {\"is_empty\"}:\n",
    "        return models.FieldCondition(key=key, is_empty=True)\n",
    "\n",
    "    # Numeric / datetime ranges\n",
    "    if op in {\"gt\", \"gte\", \"lt\", \"lte\", \"between\", \"range\"}:\n",
    "        r = _parse_range_value(value) if op in {\"between\", \"range\"} else {op: value}\n",
    "\n",
    "        if dtype in {\"datetime\", \"date\", \"timestamp\"}:\n",
    "            # DatetimeRange expects RFC3339 strings, e.g. \"2025-12-08T19:07:34Z\"\n",
    "            return models.FieldCondition(\n",
    "                key=key,\n",
    "                range=models.DatetimeRange(\n",
    "                    gt=r.get(\"gt\"),\n",
    "                    gte=r.get(\"gte\"),\n",
    "                    lt=r.get(\"lt\"),\n",
    "                    lte=r.get(\"lte\"),\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            # int/float\n",
    "            return models.FieldCondition(\n",
    "                key=key,\n",
    "                range=models.Range(\n",
    "                    gt=r.get(\"gt\"),\n",
    "                    gte=r.get(\"gte\"),\n",
    "                    lt=r.get(\"lt\"),\n",
    "                    lte=r.get(\"lte\"),\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    raise ValueError(f\"Unsupported op={op!r} for spec: {spec}\")\n",
    "\n",
    "\n",
    "def _build_filter_from_expr(expr: Spec) -> models.Filter:\n",
    "    \"\"\"\n",
    "    expr can be:\n",
    "      - atomic: {key,dtype,op,value}\n",
    "      - logical: {\"logic\":\"and|or|not\", \"clauses\":[expr, expr, ...]}\n",
    "    \"\"\"\n",
    "    logic = expr.get(\"logic\")\n",
    "    if not logic:\n",
    "        # atomic -> filter with a single must condition\n",
    "        cond = _field_condition_from_atomic(expr)\n",
    "        return models.Filter(must=[cond])\n",
    "\n",
    "    logic = logic.lower()\n",
    "    clauses = expr.get(\"clauses\", [])\n",
    "    if not isinstance(clauses, list) or not clauses:\n",
    "        raise ValueError(f\"Logical expr must have non-empty list 'clauses': {expr}\")\n",
    "\n",
    "    if logic == \"and\":\n",
    "        # Merge as must\n",
    "        must_conds = []\n",
    "        for c in clauses:\n",
    "            if c.get(\"logic\"):\n",
    "                # nested: embed as Filter in must via FilterCondition\n",
    "                must_conds.append(models.FilterCondition(filter=_build_filter_from_expr(c)))\n",
    "            else:\n",
    "                must_conds.append(_field_condition_from_atomic(c))\n",
    "        return models.Filter(must=must_conds)\n",
    "\n",
    "    if logic == \"or\":\n",
    "        should_conds = []\n",
    "        for c in clauses:\n",
    "            if c.get(\"logic\"):\n",
    "                should_conds.append(models.FilterCondition(filter=_build_filter_from_expr(c)))\n",
    "            else:\n",
    "                should_conds.append(_field_condition_from_atomic(c))\n",
    "        return models.Filter(should=should_conds, min_should=models.MinShould(min_count=1))\n",
    "\n",
    "    if logic == \"not\":\n",
    "        # NOT of a single clause or many clauses\n",
    "        must_not_conds = []\n",
    "        for c in clauses:\n",
    "            if c.get(\"logic\"):\n",
    "                must_not_conds.append(models.FilterCondition(filter=_build_filter_from_expr(c)))\n",
    "            else:\n",
    "                must_not_conds.append(_field_condition_from_atomic(c))\n",
    "        return models.Filter(must_not=must_not_conds)\n",
    "\n",
    "    raise ValueError(f\"Unsupported logic={logic!r} in expr: {expr}\")\n",
    "\n",
    "\n",
    "def adapter_specs_to_filters(\n",
    "    specs: List[Spec],\n",
    "    *,\n",
    "    mode: str = \"and\",\n",
    ") -> List[models.Filter]:\n",
    "    \"\"\"\n",
    "    Take a list of atomic specs OR logical exprs and return a list of Qdrant Filters.\n",
    "\n",
    "    Common usage:\n",
    "      - mode=\"and\": single Filter with must=[...]\n",
    "      - mode=\"or\":  single Filter with should=[...]\n",
    "      - or pass a list of logical exprs and get one Filter per expr\n",
    "\n",
    "    Returns: models.Filter\n",
    "    \"\"\"\n",
    "    mode = mode.lower()\n",
    "\n",
    "    # If user already provided logical expressions, compile each into a Filter\n",
    "    if any(\"logic\" in s for s in specs):\n",
    "        return [_build_filter_from_expr(s) for s in specs]\n",
    "\n",
    "    # Otherwise treat as atomic list and wrap according to mode\n",
    "    conds = [_field_condition_from_atomic(s) for s in specs]\n",
    "    if mode == \"and\":\n",
    "        return models.Filter(must=conds)\n",
    "    if mode == \"or\":\n",
    "        raise NotImplementedError\n",
    "    raise ValueError(f\"Unsupported mode={mode!r}. Use 'and' or 'or'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c959f9-1874-48e9-9418-a3912e37c68e",
   "metadata": {},
   "source": [
    "# QdrantVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d9523ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from typing import List, Any\n",
    "from pydantic import BaseModel\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfff01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEmbedder:\n",
    "    def __init__(self, vector_size: int, distance: str = \"cosine\"):\n",
    "        self.vector_size = vector_size\n",
    "        if distance not in {\"cosine\", \"euclidean\", \"dot\"}:\n",
    "            raise ValueError(f\"Unsupported distance metric: {distance}\")\n",
    "        self.distance = distance\n",
    "    def embed(self, text:str) -> List[Any]:\n",
    "        raise NotImplementedError\n",
    "    def embeds(self, texts:List[str]) -> List[List[Any]]:\n",
    "        raise NotImplementedError\n",
    "    def compare(self, a:List[Any], b:List[Any]) -> float:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ebcc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LshEmbedder(BaseEmbedder):\n",
    "    def __init__(self, \n",
    "                 shingle_size: int = 5,\n",
    "                 num_hashes: int = 128,\n",
    "                 bands: int = 32,\n",
    "                 seed: int = 42):\n",
    "        self.lsh = LSHMinHashEmbedder(\n",
    "            shingle_size=shingle_size,\n",
    "            num_hashes=num_hashes,\n",
    "            bands=bands,\n",
    "            seed=seed\n",
    "        )\n",
    "        super().__init__(vector_size=num_hashes, distance=\"cosine\")\n",
    "\n",
    "    def embed(self, text: str) -> List[int]:\n",
    "        lsh_emb = self.lsh.embed(text)\n",
    "        return lsh_emb.signature\n",
    "\n",
    "    def embeds(self, texts: List[str]) -> Iterable[List[int]]:\n",
    "        for text in texts:\n",
    "            yield self.embed(text)\n",
    "\n",
    "    def compare(self, a: List[int], b: List[int]) -> float:\n",
    "        def jaccard(sig_a: List[int], sig_b: List[int]) -> float:\n",
    "            if len(sig_a) != len(sig_b):\n",
    "                raise ValueError(\"Signatures must be of the same length for comparison.\")\n",
    "            matches = sum(1 for x, y in zip(sig_a, sig_b) if x == y)\n",
    "            return matches / len(sig_a)\n",
    "        return jaccard(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ee7f45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Insertable(BaseModel):\n",
    "    text: str\n",
    "    metadata: dict\n",
    "    insert_timestamp: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2eca347e-fe0b-44c0-9b3e-fd613e8194fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytz import timezone\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name,\n",
    "        embedder: BaseEmbedder,\n",
    "        url=\"http://localhost:6333\",\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.embedder = embedder\n",
    "        self.url = url\n",
    "        self.client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "        self.create_collection_if_not_exists(\n",
    "                vector_size=embedder.vector_size,\n",
    "                distance=embedder.distance\n",
    "            )\n",
    "\n",
    "    def create_collection_if_not_exists(self, \n",
    "                                        vector_size: int, \n",
    "                                        distance: str = \"Cosine\"):\n",
    "        \n",
    "        if distance == \"cosine\":\n",
    "            dist = models.Distance.COSINE\n",
    "        elif distance == \"euclidean\":\n",
    "            dist = models.Distance.EUCLIDEAN\n",
    "        elif distance == \"dot\":\n",
    "            dist = models.Distance.DOT\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "\n",
    "        if not self.client.collection_exists(collection_name=self.collection_name):\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=vector_size,\n",
    "                    distance=dist\n",
    "                )\n",
    "            )\n",
    "            print(f\"Created collection: {self.collection_name}\")\n",
    "        else:\n",
    "            print(f\"Collection {self.collection_name} already exists.\")\n",
    "    \n",
    "    def delete_collection_if_exists(self):\n",
    "        if self.client.collection_exists(collection_name=self.collection_name):\n",
    "            self.client.delete_collection(collection_name=self.collection_name)\n",
    "            print(f\"Deleted collection: {self.collection_name}\")\n",
    "\n",
    "    def _hash(self, insertable: Insertable) -> str:\n",
    "        stringifoed_obj = f\"{insertable.text}-{insertable.metadata}\"\n",
    "        return hashlib.md5(stringifoed_obj.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def inserts(self, objects: List[Dict]):\n",
    "        # Validate inputs to check 'text' and 'metadata' keys\n",
    "        insertables = [Insertable(**obj) for obj in objects]\n",
    "        texts = [obj.text for obj in insertables]\n",
    "        for insertable, embedding in zip(insertables, self.embedder.embeds(texts)):\n",
    "            _id = self._hash(insertable)\n",
    "            # Current timestamp\n",
    "            ISO_8601_UTC_REGEX = r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?Z$\"\n",
    "\n",
    "            if insertable.insert_timestamp is None:\n",
    "                insertable.insert_timestamp = get_time()\n",
    "            else:\n",
    "                if not re.match(ISO_8601_UTC_REGEX, insertable.insert_timestamp):\n",
    "                    raise ValueError(\n",
    "                        f\"insert_timestamp must be in ISO 8601 UTC format (…Z): {insertable.insert_timestamp}\"\n",
    "                    )\n",
    "\n",
    "            print(f\"Inserting ID: {_id}\")\n",
    "            self.client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=[\n",
    "                    {\n",
    "                        \"id\": self._hash(insertable),\n",
    "                        \"vector\": embedding,\n",
    "                        \"payload\": {\n",
    "                            **insertable.metadata,\n",
    "                            \"insert_timestamp\": insertable.insert_timestamp,\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def insert(self, obj: Dict):\n",
    "        objs = [obj]\n",
    "        self.inserts(objs)\n",
    "\n",
    "    def search(self, query: str, filter_: Optional[models.Filter] = None, top_k: int = 5):\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        results = self.client.query_points(\n",
    "            collection_name=self.collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_,\n",
    "            limit=top_k,\n",
    "            with_payload=True\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def search_batch(self, queries: List[str], filters: List[Optional[models.Filter]] = None, top_k: int = 5):\n",
    "        assert filters is None or len(filters) == len(queries), \"Filters length must match queries length\"\n",
    "            \n",
    "        search_queries = [\n",
    "            models.QueryRequest(\n",
    "                query=self.embedder.embed(q),\n",
    "                filter=f if filters else None,\n",
    "                limit=top_k,\n",
    "                with_payload=True\n",
    "            ) for q, f in zip(queries, filters or [None]*len(queries))\n",
    "        ]\n",
    "\n",
    "        res = self.client.query_batch_points(\n",
    "            collection_name=self.collection_name,\n",
    "            requests=search_queries\n",
    "        )\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def scroll(self, filters: Optional[models.Filter] = None, batch_size: int = 10):\n",
    "        scroll_result = self.client.scroll(\n",
    "            collection_name=self.collection_name,\n",
    "            scroll_filter=filters,\n",
    "            limit=batch_size,\n",
    "            with_payload=True\n",
    "        )\n",
    "        return scroll_result\n",
    "    \n",
    "    def delete_conditional(self, filters: Optional[models.Filter] = None):\n",
    "        res = self.client.delete_points(\n",
    "            collection_name=self.collection_name,\n",
    "            points_selector=models.PointsSelector(filter=filters)\n",
    "        ) \n",
    "        return res\n",
    "    \n",
    "    def update_payload_conditional(self, new_payload: Dict[str, Any], filters: Optional[models.Filter] = None):\n",
    "        self.client.set_payload(\n",
    "            collection_name=self.collection_name,\n",
    "            payload=new_payload,\n",
    "            points=models.PointsSelector(filter=filters)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b994f73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection test_collection already exists.\n"
     ]
    }
   ],
   "source": [
    "lsh_vector_store = VectorStore(\n",
    "    collection_name=\"test_collection\",\n",
    "    embedder=LshEmbedder(\n",
    "        shingle_size=5,\n",
    "        num_hashes=128,\n",
    "        bands=32,\n",
    "        seed=123\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c55d1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsh_vector_store.delete_collection_if_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f29bd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = [\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-13T12:00:01Z user_id=123e4567-e89b-12d3-a456-426614174000 request took 153ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-1\", \"timestamp\": \"2026-01-13T12:00:01Z\", \"severity\": \"ERROR\", \"textPayload\": \"Cannot assign requested address XYZ\"},\n",
    "        \"insert_timestamp\": \"2026-01-13T12:05:00Z\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-13T12:00:09Z\", \"severity\": \"ERROR\", \"textPayload\": \"Cannot assign requested address ABC\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-14T12:00:09Z\", \"severity\": \"INFO\", \"textPayload\": \"User login successful\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-14T12:00:09Z\", \"severity\": \"WARNING\", \"textPayload\": \"Cannot assign requested address DED\"}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "25c450cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting ID: 00b6968f3ce972617626f646d7288bd4\n",
      "Inserting ID: d8c48942a4da398e595f15c9aefb735c\n",
      "Inserting ID: 662691f5f1213832fad725003284f1a5\n",
      "Inserting ID: cd634c54630985964c5403ae81abec5f\n"
     ]
    }
   ],
   "source": [
    "lsh_vector_store.inserts(sample_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c4939299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-01-18 08:55:59.906812Z'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.datetime.now(datetime.UTC)).replace(\"+00:00\", \"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "824a0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = [\n",
    "    {\"key\": \"pod_name\", \"dtype\": \"string\", \"op\": \"equals\", \"value\": \"pod-1\"},\n",
    "    # {\"key\": \"timestamp\", \"dtype\": \"datetime\", \"op\": \"between\",\"value\": \"2026-01-13T00:00:00Z, 2026-01-13T20:00:00Z\"},\n",
    "    # {\"key\": \"severity\", \"dtype\": \"string\", \"op\": \"in\", \"value\": [\"ERROR\", \"CRITICAL\"]},\n",
    "    # {\"key\": \"textPayload\", \"dtype\": \"text\", \"op\": \"contains\", \"value\": \"ABC\"},\n",
    "]\n",
    "filter_ = adapter_specs_to_filters(specs, mode=\"and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c4fc4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = lsh_vector_store.search(\n",
    "    query=\"ERROR 2026-01-16T12:00:05Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\",\n",
    "    filter_=filter_,\n",
    "    top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e78063d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='00b6968f-3ce9-7261-7626-f646d7288bd4', version=1, score=0.6531415, payload={'pod_name': 'pod-1', 'timestamp': '2026-01-13T12:00:01Z', 'severity': 'ERROR', 'textPayload': 'Cannot assign requested address XYZ', 'insert_timestamp': '2026-01-13T12:05:00Z'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c23584c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QueryResponse(points=[ScoredPoint(id='d8c48942-a4da-398e-595f-15c9aefb735c', version=2, score=0.7240081, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-13T12:00:09Z', 'severity': 'ERROR', 'textPayload': 'Cannot assign requested address ABC', 'insert_timestamp': '2026-01-18T06:11:28.746597+00:00Z'}, vector=None, shard_key=None, order_value=None)]),\n",
       " QueryResponse(points=[ScoredPoint(id='662691f5-f121-3832-fad7-25003284f1a5', version=3, score=1.0, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'User login successful', 'insert_timestamp': '2026-01-18T06:11:28.756365+00:00Z'}, vector=None, shard_key=None, order_value=None)])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh_vector_store.search_batch(\n",
    "    [\"ERROR 2026-01-16T12:00:05Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\",\n",
    "     \"INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user\"],\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b56340f",
   "metadata": {},
   "source": [
    "# General Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3722eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10bf5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseConnector(ABC):\n",
    "    @abstractmethod\n",
    "    def connect(self, username: str, password: str, database: str, host: str, port: int):\n",
    "        pass    \n",
    "    @abstractmethod\n",
    "    def execute_and_return_result(self, query: str) -> Any:\n",
    "        pass\n",
    "\n",
    "    def _validate_dbargs(self, dbargs):\n",
    "        required_keys = ['username', 'password', 'database', 'host', 'port']\n",
    "        for key in required_keys:\n",
    "            if key not in dbargs:\n",
    "                raise ValueError(f\"Missing required database argument: {key}\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de0f2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgressConnector(BaseConnector):\n",
    "    def __init__(self, dbargs: Dict[str, Any]):\n",
    "        self._validate_dbargs(dbargs)\n",
    "        self.dbargs = dbargs\n",
    "        self.connection = None\n",
    "        try:\n",
    "            self.connect(\n",
    "                username=dbargs['username'],\n",
    "                password=dbargs['password'],\n",
    "                database=dbargs['database'],\n",
    "                host=dbargs['host'],\n",
    "                port=dbargs['port']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to the database: {e}\")\n",
    "            self.connection = None\n",
    "\n",
    "    def connect(self, username: str, password: str, database: str, host: str, port: int):\n",
    "        self.connection = psycopg2.connect(\n",
    "            dbname=database,\n",
    "            user=username,\n",
    "            password=password,\n",
    "            host=host,\n",
    "            port=port\n",
    "        )\n",
    "    \n",
    "    def execute_and_return_result(self, query: str, params=None):\n",
    "        try:\n",
    "            with self.connection.cursor() as cursor:\n",
    "                if params:\n",
    "                    cursor.execute(query, params)\n",
    "                else:\n",
    "                    cursor.execute(query)\n",
    "\n",
    "                if cursor.description:\n",
    "                    result = cursor.fetchall()\n",
    "                else:\n",
    "                    result = []\n",
    "                \n",
    "                self.connection.commit()\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            try:\n",
    "                self.connection.rollback()\n",
    "            except Exception as rollback_error:\n",
    "                print(f\"Error during rollback: {rollback_error}\")\n",
    "            raise e\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "            self.connection = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5458cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_connector(db_type: str, username: str, password: str, database: str, host: str, port: int) -> BaseConnector:\n",
    "    dbargs = {\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'database': database,\n",
    "        'host': host,\n",
    "        'port': port\n",
    "    }\n",
    "    if db_type.lower() == 'postgresql':\n",
    "        return PostgressConnector(dbargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported database type: {db_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "266c20c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PG_CONNECTOR = get_database_connector(\n",
    "    db_type=\"postgresql\",\n",
    "    username=\"admin\",\n",
    "    password=\"admin\",\n",
    "    database=\"mydatabase\",\n",
    "    host=\"localhost\",\n",
    "    port=5432\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d288e954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PostgreSQL 18.1 (Debian 18.1-1.pgdg13+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit',)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_PG_CONNECTOR.execute_and_return_result(\"SELECT version();\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4746d590",
   "metadata": {},
   "source": [
    "# Log Ingestion Hybrid DB Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogIngestionHtbridHandler:\n",
    "    def __init__(self, \n",
    "                 qdrant_lsh_collection_name: str,\n",
    "                 qdrant_lsh_shingle_size: int,\n",
    "                 qdrant_lsh_num_hashes: int,\n",
    "                 qdrant_lsh_bands: int,\n",
    "                 qdrant_lsh_seed: int,\n",
    "                 pg_lookupdb_username: str,\n",
    "                 pg_lookupdb_password: str,\n",
    "                 pg_lookupdb_database: str,\n",
    "                 pg_lookupdb_host: str,\n",
    "                 pg_lookupdb_port: int,\n",
    "                 pg_lookupdb_table_name: str,\n",
    "                 insert_sim_threshold: float,\n",
    "                 sim_sync_batch_size: int\n",
    "                 ):\n",
    "        \n",
    "        self.vector_store = VectorStore(\n",
    "            collection_name=qdrant_lsh_collection_name,\n",
    "            embedder=LshEmbedder(\n",
    "                shingle_size=qdrant_lsh_shingle_size,\n",
    "                num_hashes=qdrant_lsh_num_hashes,\n",
    "                bands=qdrant_lsh_bands,\n",
    "                seed=qdrant_lsh_seed\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.db_connector = get_database_connector(\n",
    "            db_type=\"postgresql\",\n",
    "            username=pg_lookupdb_username,\n",
    "            password=pg_lookupdb_password,\n",
    "            database=pg_lookupdb_database,\n",
    "            host=pg_lookupdb_host,\n",
    "            port=pg_lookupdb_port\n",
    "        )\n",
    "        \n",
    "        self.pg_lookupdb_table_name = pg_lookupdb_table_name\n",
    "        self.insert_sim_threshold = insert_sim_threshold\n",
    "        self.sim_sync_batch_size = sim_sync_batch_size\n",
    "\n",
    "        self._warmup_lookup_db()\n",
    "\n",
    "    \n",
    "    def _warmup_lookup_db(self):\n",
    "        # Simple query to test connection\n",
    "        result = self.db_connector.execute_and_return_result(\"SELECT 1;\")\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.pg_lookupdb_table_name} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            closest_log_id TEXT NOT NULL,\n",
    "            similarity FLOAT NOT NULL,\n",
    "            timestamp TIMESTAMP NOT NULL,\n",
    "            location TEXT\n",
    "        );\n",
    "        \"\"\"\n",
    "        res = self.db_connector.execute_and_return_result(create_table_query)\n",
    "        return res\n",
    "\n",
    "    def _insert_into_lookup_db(self, \n",
    "                               log_id: str, \n",
    "                               closest_log_id: str, \n",
    "                               similarity: float, \n",
    "                               timestamp: str, \n",
    "                               location: str):\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {self.pg_lookupdb_table_name} (id, closest_log_id, similarity, timestamp, location)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "        ON CONFLICT (id) DO NOTHING;\n",
    "        \"\"\"\n",
    "        params = (log_id, closest_log_id, similarity, timestamp, location)\n",
    "        self.db_connector.execute_and_return_result(insert_query, params)\n",
    "                 \n",
    "\n",
    "    def insert_logs(self, log_entries: List[Dict]):\n",
    "        # Check with vector store for near-duplicates\n",
    "        texts = [entry['text'] for entry in log_entries]\n",
    "        search_results = self.vector_store.search_batch(\n",
    "            queries=texts,\n",
    "            top_k=1\n",
    "        )\n",
    "\n",
    "        if search_results is None or len(search_results) == 0:\n",
    "            search_results = [None]*len(log_entries)\n",
    "            \n",
    "        vector_store_inserts = []\n",
    "        for entry, result in zip(log_entries, search_results):\n",
    "            entry['insert_timestamp'] = get_time()\n",
    "            entry[\"metadata\"][\"sim_sync\"] = False\n",
    "            hashval = self.vector_store._hash(Insertable(text=entry['text'], metadata=entry.get('metadata', {})))\n",
    "            if result.points:\n",
    "                top_point = result.points[0]\n",
    "                sim = top_point.score\n",
    "\n",
    "                # sim = self.vector_store.embedder.compare(\n",
    "                #     a=self.vector_store.embedder.embed(entry['text']),\n",
    "                #     b=top_point.vector\n",
    "                # )\n",
    "                print(f\"{entry['text'][:30]}... <=> Top match ID: {top_point.id}/{top_point.payload.get('text','')[:30]}... with similarity: {sim}\")\n",
    "                \n",
    "                if sim >= self.insert_sim_threshold:\n",
    "                    print(f\"Skipping insert for log (ID: {hashval}) due to high similarity ({sim}) with existing log (ID: {top_point.id})\")\n",
    "                    # Only insert into lookup DB\n",
    "                    self._insert_into_lookup_db(\n",
    "                        log_id=hashval,\n",
    "                        closest_log_id=str(top_point.id),\n",
    "                        similarity=sim,\n",
    "                        timestamp=entry.get('metadata', {}).get('timestamp', ''),\n",
    "                        location=entry.get('metadata', {}).get('pod_name', '')\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"Inserting new log (ID: {hashval}) into vector store and lookup DB\")\n",
    "                    # Insert into vector store and lookup DB ( Considered new log)\n",
    "                    self._insert_into_lookup_db(\n",
    "                        log_id=hashval,\n",
    "                        closest_log_id=hashval,\n",
    "                        similarity=1.0,\n",
    "                        timestamp=entry.get('metadata', {}).get('timestamp', ''),\n",
    "                        location=entry.get('metadata', {}).get('pod_name', '')\n",
    "                    )\n",
    "                    vector_store_inserts.append(entry)\n",
    "\n",
    "            else:\n",
    "                print(f\"No existing points found. Inserting new log (ID: {hashval}) into vector store and lookup DB\")\n",
    "                self._insert_into_lookup_db(\n",
    "                    log_id=hashval,\n",
    "                    closest_log_id=hashval,\n",
    "                    similarity=1.0,\n",
    "                    timestamp=entry.get('metadata', {}).get('timestamp', ''),\n",
    "                    location=entry.get('metadata', {}).get('pod_name', '')\n",
    "                )\n",
    "                vector_store_inserts.append(entry)\n",
    "\n",
    "        if vector_store_inserts:\n",
    "            self.vector_store.inserts(vector_store_inserts)\n",
    "\n",
    "    def search(self, query: str, filter_: Optional[List[Dict]] = None, top_k: int = 5):\n",
    "        qdrant_filter = adapter_specs_to_filters(filter_) if filter_ else None\n",
    "        results = self.vector_store.search(\n",
    "            query=query,\n",
    "            filter_=qdrant_filter,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def scroll(self, filter_: Optional[List[Dict]] = None, batch_size: int = 10):\n",
    "        qdrant_filter = adapter_specs_to_filters(filter_) if filter_ else None\n",
    "        results = self.vector_store.scroll(\n",
    "            filters=qdrant_filter,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        return results\n",
    "    \n",
    "    def find_near_occurrences(self, log_id):\n",
    "        # Fetch log entry from lookup DB\n",
    "        fetch_query = f\"\"\"\n",
    "        SELECT closest_log_id, similarity FROM {self.pg_lookupdb_table_name}\n",
    "        WHERE id = %s;\n",
    "        \"\"\"\n",
    "        params = (log_id,)\n",
    "        result = self.db_connector.execute_and_return_result(fetch_query, params)\n",
    "        return result\n",
    "\n",
    "    def clear(self):\n",
    "        self.vector_store.delete_collection_if_exists()\n",
    "        delete_query = f\"DELETE FROM {self.pg_lookupdb_table_name};\"\n",
    "        self.db_connector.execute_and_return_result(delete_query)\n",
    "\n",
    "    def sync_similarty(self):\n",
    "        # Fetch all entries from vector store where sim_sync is False\n",
    "        filter = adapter_specs_to_filters([\n",
    "            {\"key\": \"sim_sync\", \"dtype\": \"boolean\", \"op\": \"equals\", \"value\": False}\n",
    "        ], mode=\"and\")\n",
    "\n",
    "        unsync_points = self.vector_store.scroll(\n",
    "            filters=filter,\n",
    "            batch_size=self.sim_sync_batch_size\n",
    "        )[0]\n",
    "\n",
    "        edges_to_update = []\n",
    "\n",
    "        texts = [p.payload.get(\"text\", \"\") for p in unsync_points]\n",
    "        search_results = self.vector_store.search_batch(\n",
    "            queries=texts,\n",
    "            filters=[filter]*len(texts),\n",
    "            top_k=1\n",
    "        )\n",
    "\n",
    "        return unsync_points, search_results, texts\n",
    "        print(f\"Processing batch of {len(unsync_points)} vs {len(search_results)}\")\n",
    "\n",
    "        for point, result in zip(unsync_points, search_results):\n",
    "            if result.points and len(result.points) > 0:\n",
    "                top_point = result.points[0]\n",
    "                sim = top_point.score\n",
    "                print(f\"Log ID: {point.id} <=> Top match ID: {top_point.id} with similarity: {sim}\")\n",
    "                edges_to_update.append([point.id, str(top_point.id), sim])\n",
    "\n",
    "        return edges_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "3f3efae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection log_lsh_collection already exists.\n"
     ]
    }
   ],
   "source": [
    "log_ingestion_handler = LogIngestionHtbridHandler(\n",
    "    qdrant_lsh_collection_name=\"log_lsh_collection\",\n",
    "    qdrant_lsh_shingle_size=3,\n",
    "    qdrant_lsh_num_hashes=256,\n",
    "    qdrant_lsh_bands=32,\n",
    "    qdrant_lsh_seed=123,\n",
    "    pg_lookupdb_username=\"admin\",\n",
    "    pg_lookupdb_password=\"admin\",\n",
    "    pg_lookupdb_database=\"mydatabase\",\n",
    "    pg_lookupdb_host=\"localhost\",\n",
    "    pg_lookupdb_port=5432,\n",
    "    pg_lookupdb_table_name=\"log_lookup_table\",\n",
    "    insert_sim_threshold=0.8,\n",
    "    sim_sync_batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "b670b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_ingestion_handler.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6f3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "46cfee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log_entries = [\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-15T12:00:01Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-3\", \"timestamp\": \"2026-01-15T12:00:01Z\", \"severity\": \"ERROR\", \"textPayload\": \"ERROR 2026-01-15T12:00:01Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-14T12:00:09Z\", \"severity\": \"INFO\", \"textPayload\": \"INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-14T12:00:09Z\", \"severity\": \"WARNING\", \"textPayload\": \"ERROR 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-13T12:00:01Z user_id=123e4567-e89b-12d3-a456-426614174000 request took 153ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-1\", \"timestamp\": \"2026-01-13T12:00:01Z\", \"severity\": \"ERROR\", \"textPayload\": \"ERROR 2026-01-13T12:00:01Z user_id=123e4567-e89b-12d3-a456-426614174000 request took 153ms\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\",\n",
    "        \"metadata\": {\"pod_name\": \"pod-2\", \"timestamp\": \"2026-01-13T12:00:09Z\", \"severity\": \"ERROR\", \"textPayload\": \"ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\"}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "b879d910",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedResponse",
     "evalue": "Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 128, got 256\"},\"time\":0.000965167}'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnexpectedResponse\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[379]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlog_ingestion_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_log_entries\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mLogIngestionHtbridHandler.insert_logs\u001b[39m\u001b[34m(self, log_entries)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minsert_logs\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_entries: List[Dict]):\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Check with vector store for near-duplicates\u001b[39;00m\n\u001b[32m     76\u001b[39m     texts = [entry[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m log_entries]\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     search_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m search_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(search_results) == \u001b[32m0\u001b[39m:\n\u001b[32m     83\u001b[39m         search_results = [\u001b[38;5;28;01mNone\u001b[39;00m]*\u001b[38;5;28mlen\u001b[39m(log_entries)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[207]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mVectorStore.search_batch\u001b[39m\u001b[34m(self, queries, filters, top_k)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filters) == \u001b[38;5;28mlen\u001b[39m(queries), \u001b[33m\"\u001b[39m\u001b[33mFilters length must match queries length\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m search_queries = [\n\u001b[32m    107\u001b[39m     models.QueryRequest(\n\u001b[32m    108\u001b[39m         query=\u001b[38;5;28mself\u001b[39m.embedder.embed(q),\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m     ) \u001b[38;5;28;01mfor\u001b[39;00m q, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(queries, filters \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m]*\u001b[38;5;28mlen\u001b[39m(queries))\n\u001b[32m    113\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_batch_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_queries\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_client.py:261\u001b[39m, in \u001b[36mQdrantClient.query_batch_points\u001b[39m\u001b[34m(self, collection_name, requests, consistency, timeout, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cloud_inference \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inference_inspector.inspect(requests):\n\u001b[32m    255\u001b[39m     requests = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m    256\u001b[39m         \u001b[38;5;28mself\u001b[39m._embed_models(\n\u001b[32m    257\u001b[39m             requests, is_query=\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size=\u001b[38;5;28mself\u001b[39m.local_inference_batch_size\n\u001b[32m    258\u001b[39m         )\n\u001b[32m    259\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_batch_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py:587\u001b[39m, in \u001b[36mQdrantRemote.query_batch_points\u001b[39m\u001b[34m(self, collection_name, requests, consistency, timeout, **kwargs)\u001b[39m\n\u001b[32m    580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    581\u001b[39m         models.QueryResponse(\n\u001b[32m    582\u001b[39m             points=[GrpcToRest.convert_scored_point(hit) \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m r.result]\n\u001b[32m    583\u001b[39m         )\n\u001b[32m    584\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m grpc_res.result\n\u001b[32m    585\u001b[39m     ]\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m587\u001b[39m     http_res: \u001b[38;5;28mlist\u001b[39m[models.QueryResponse] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_batch_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_request_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQueryRequestBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearches\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequests\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.result\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m http_res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mQuery batch returned None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m http_res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api/search_api.py:766\u001b[39m, in \u001b[36mSyncSearchApi.query_batch_points\u001b[39m\u001b[34m(self, collection_name, consistency, timeout, query_request_batch)\u001b[39m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_batch_points\u001b[39m(\n\u001b[32m    757\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    758\u001b[39m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    761\u001b[39m     query_request_batch: m.QueryRequestBatch = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    762\u001b[39m ) -> m.InlineResponse20022:\n\u001b[32m    763\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    764\u001b[39m \u001b[33;03m    Universally query points in batch. This endpoint covers all capabilities of search, recommend, discover, filters. But also enables hybrid and multi-stage queries.\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_for_query_batch_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_request_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api/search_api.py:147\u001b[39m, in \u001b[36m_SearchApi._build_for_query_batch_points\u001b[39m\u001b[34m(self, collection_name, consistency, timeout, query_request_batch)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[32m    146\u001b[39m     headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInlineResponse20022\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[33;43m/points/query/batch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:95\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, type_, method, url, path_params, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m request = \u001b[38;5;28mself\u001b[39m._client.build_request(method, url, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:130\u001b[39m, in \u001b[36mApiClient.send\u001b[39m\u001b[34m(self, request, type_)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedResponse.for_response(response)\n",
      "\u001b[31mUnexpectedResponse\u001b[39m: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 128, got 256\"},\"time\":0.000965167}'"
     ]
    }
   ],
   "source": [
    "log_ingestion_handler.insert_logs(sample_log_entries[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "1cc38f12",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedResponse",
     "evalue": "Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 128, got 256\"},\"time\":0.002704625}'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnexpectedResponse\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[375]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlog_ingestion_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mERROR 2026-01-16T12:00:05Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# filter_=[\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     {\"key\": \"pod_name\", \"dtype\": \"string\", \"op\": \"equals\", \"value\": \"pod-3\"},\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#     {\"key\": \"severity\", \"dtype\": \"string\", \"op\": \"equals\", \"value\": \"ERROR\"}\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ],\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m.points\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[360]\u001b[39m\u001b[32m, line 138\u001b[39m, in \u001b[36mLogIngestionHtbridHandler.search\u001b[39m\u001b[34m(self, query, filter_, top_k)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, filter_: Optional[List[Dict]] = \u001b[38;5;28;01mNone\u001b[39;00m, top_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m):\n\u001b[32m    137\u001b[39m     qdrant_filter = adapter_specs_to_filters(filter_) \u001b[38;5;28;01mif\u001b[39;00m filter_ \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqdrant_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[207]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mVectorStore.search\u001b[39m\u001b[34m(self, query, filter_, top_k)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, filter_: Optional[models.Filter] = \u001b[38;5;28;01mNone\u001b[39;00m, top_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m):\n\u001b[32m     93\u001b[39m     query_embedding = \u001b[38;5;28mself\u001b[39m.embedder.embed(query)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilter_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_client.py:423\u001b[39m, in \u001b[36mQdrantClient.query_points\u001b[39m\u001b[34m(self, collection_name, query, using, prefetch, query_filter, search_params, limit, offset, with_payload, with_vectors, score_threshold, lookup_from, consistency, shard_key_selector, timeout, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m             prefetch = (\n\u001b[32m    410\u001b[39m                 \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m    411\u001b[39m                     \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    420\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    421\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefetch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefetch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_filter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_vectors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_vectors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlookup_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlookup_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_key_selector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshard_key_selector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/qdrant_remote.py:538\u001b[39m, in \u001b[36mQdrantRemote.query_points\u001b[39m\u001b[34m(self, collection_name, query, using, prefetch, query_filter, search_params, limit, offset, with_payload, with_vectors, score_threshold, lookup_from, consistency, shard_key_selector, timeout, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m     lookup_from = GrpcToRest.convert_lookup_location(lookup_from)\n\u001b[32m    523\u001b[39m query_request = models.QueryRequest(\n\u001b[32m    524\u001b[39m     shard_key=shard_key_selector,\n\u001b[32m    525\u001b[39m     prefetch=prefetch,\n\u001b[32m   (...)\u001b[39m\u001b[32m    535\u001b[39m     lookup_from=lookup_from,\n\u001b[32m    536\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m query_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m result: models.QueryResponse | \u001b[38;5;28;01mNone\u001b[39;00m = query_result.result\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mSearch returned None\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api/search_api.py:783\u001b[39m, in \u001b[36mSyncSearchApi.query_points\u001b[39m\u001b[34m(self, collection_name, consistency, timeout, query_request)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_points\u001b[39m(\n\u001b[32m    774\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    775\u001b[39m     collection_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m     query_request: m.QueryRequest = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    779\u001b[39m ) -> m.InlineResponse20021:\n\u001b[32m    780\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[33;03m    Universally query points. This endpoint covers all capabilities of search, recommend, discover, filters. But also enables hybrid and multi-stage queries.\u001b[39;00m\n\u001b[32m    782\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_for_query_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconsistency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    786\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api/search_api.py:181\u001b[39m, in \u001b[36m_SearchApi._build_for_query_points\u001b[39m\u001b[34m(self, collection_name, consistency, timeout, query_request)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m headers:\n\u001b[32m    180\u001b[39m     headers[\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mapplication/json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mInlineResponse20021\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/collections/\u001b[39;49m\u001b[38;5;132;43;01m{collection_name}\u001b[39;49;00m\u001b[33;43m/points/query\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:95\u001b[39m, in \u001b[36mApiClient.request\u001b[39m\u001b[34m(self, type_, method, url, path_params, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mint\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m request = \u001b[38;5;28mself\u001b[39m._client.build_request(method, url, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/programming/cloud-ai-police/HTLL-IR-framework/.venv/lib/python3.12/site-packages/qdrant_client/http/api_client.py:130\u001b[39m, in \u001b[36mApiClient.send\u001b[39m\u001b[34m(self, request, type_)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    129\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ResponseHandlingException(e)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedResponse.for_response(response)\n",
      "\u001b[31mUnexpectedResponse\u001b[39m: Unexpected Response: 400 (Bad Request)\nRaw response content:\nb'{\"status\":{\"error\":\"Wrong input: Vector dimension error: expected dim: 128, got 256\"},\"time\":0.002704625}'"
     ]
    }
   ],
   "source": [
    "log_ingestion_handler.search(\n",
    "    query=\"ERROR 2026-01-16T12:00:05Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\",\n",
    "    # filter_=[\n",
    "    #     {\"key\": \"pod_name\", \"dtype\": \"string\", \"op\": \"equals\", \"value\": \"pod-3\"},\n",
    "    #     {\"key\": \"severity\", \"dtype\": \"string\", \"op\": \"equals\", \"value\": \"ERROR\"}\n",
    "    # ],\n",
    "    top_k=2\n",
    ").points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "ee0f3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_ingestion_handler.db_connector.execute_and_return_result(\"SELECT * FROM log_lookup_table;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d48d1e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = log_ingestion_handler.vector_store.embedder.embed(\"ERROR 2026-01-15T12:00:01Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms\")\n",
    "v2 = log_ingestion_handler.vector_store.embedder.embed(\"ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms\")\n",
    "cmp = log_ingestion_handler.vector_store.embedder.compare(v1, v2)\n",
    "cmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "1f827b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = log_ingestion_handler.sync_similarty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "9f9ccff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(id='061662e3-339f-0f64-3442-5a6b39de2f65', payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'WARNING', 'textPayload': 'ERROR 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.281270Z'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='093a7511-84a4-8557-7e3e-2ac03af40441', payload={'pod_name': 'pod-1', 'timestamp': '2026-01-13T12:00:01Z', 'severity': 'ERROR', 'textPayload': 'ERROR 2026-01-13T12:00:01Z user_id=123e4567-e89b-12d3-a456-426614174000 request took 153ms', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.283414Z'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='6ac3abf6-b388-0078-4054-c39562712580', payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='e0d11f33-6814-67d3-6264-6ea31b515ef7', payload={'pod_name': 'pod-2', 'timestamp': '2026-01-13T12:00:09Z', 'severity': 'ERROR', 'textPayload': 'ERROR 2026-01-13T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 request took 149ms', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.285535Z'}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='ec3f305b-964b-9bfc-4fea-ec791db61a04', payload={'pod_name': 'pod-3', 'timestamp': '2026-01-15T12:00:01Z', 'severity': 'ERROR', 'textPayload': 'ERROR 2026-01-15T12:00:01Z user_id=323e4567-e89b-12d3-a456-4266141731314888 request took 150ms', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.272371Z'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "f693e164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QueryResponse(points=[ScoredPoint(id='6ac3abf6-b388-0078-4054-c39562712580', version=2, score=0.7341126, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None)]),\n",
       " QueryResponse(points=[ScoredPoint(id='6ac3abf6-b388-0078-4054-c39562712580', version=2, score=0.7341126, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None)]),\n",
       " QueryResponse(points=[ScoredPoint(id='6ac3abf6-b388-0078-4054-c39562712580', version=2, score=0.7341126, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None)]),\n",
       " QueryResponse(points=[ScoredPoint(id='6ac3abf6-b388-0078-4054-c39562712580', version=2, score=0.7341126, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None)]),\n",
       " QueryResponse(points=[ScoredPoint(id='6ac3abf6-b388-0078-4054-c39562712580', version=2, score=0.7341126, payload={'pod_name': 'pod-2', 'timestamp': '2026-01-14T12:00:09Z', 'severity': 'INFO', 'textPayload': 'INFO 2026-01-14T12:00:09Z user_id=223e4567-e89b-12d3-a456-426614174999 passed user', 'sim_sync': False, 'insert_timestamp': '2026-01-18T09:16:09.278478Z'}, vector=None, shard_key=None, order_value=None)])]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htll-ir-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
