{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33604932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "env_path = f\"{current_dir}/.env\"\n",
    "load_dotenv(dotenv_path=env_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2392bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY loaded successfully.\n",
      "GOOGLE_APPLICATION_CREDENTIALS loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if gemini API key is loaded\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key:\n",
    "    print(\"GEMINI_API_KEY loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GEMINI_API_KEY.\")\n",
    "\n",
    "# Check for GOOGLE_APPLICATION_CREDENTIALS\n",
    "if os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GOOGLE_APPLICATION_CREDENTIALS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efd2fe",
   "metadata": {},
   "source": [
    "## Some Basic Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91dc428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv add google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"]=\"true\"\n",
    "from google import genai\n",
    "client = genai.Client()\n",
    "\n",
    "# Following this docs: https://ai.google.dev/gemini-api/docs/quickstart\n",
    "# Pricing: https://ai.google.dev/gemini-api/docs/pricing\n",
    "# Avaialable models: https://ai.google.dev/gemini-api/docs/models\n",
    "# Writing good prompts: https://ai.google.dev/gemini-api/docs/prompting-strategies\n",
    "# Langchain integration: https://ai.google.dev/gemini-api/docs/langgraph-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI works by **learning from data to identify patterns and make intelligent decisions or predictions.**\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17921e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At its simplest level, **Artificial Intelligence (AI) works by identifying patterns in massive amounts of data.**\n",
      "\n",
      "To understand how it works, it helps to compare it to traditional computer programming.\n",
      "\n",
      "### 1. Traditional Programming vs. AI\n",
      "*   **Traditional Programming:** A human writes a specific set of instructions (rules). *“If the user clicks this button, show this image.”* The computer follows the recipe exactly.\n",
      "*   **AI (Machine Learning):** Instead of giving the computer rules, we give it **data** and the **result** we want. The computer then \"figures out\" the rules itself by finding mathematical patterns.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. The Three Pillars of AI\n",
      "For AI to work, it needs three things:\n",
      "1.  **Data (The Fuel):** This can be text, images, sensor readings, or clicks. AI needs thousands (or billions) of examples to learn.\n",
      "2.  **Algorithms (The Engine):** These are mathematical formulas. The most common type today is the **Neural Network**, which is loosely inspired by how neurons in the human brain fire.\n",
      "3.  **Computing Power (The Factory):** Processing billions of calculations requires massive amounts of hardware (GPUs).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. How the \"Learning\" Process Works\n",
      "Let’s use an example: **Teaching an AI to recognize a cat.**\n",
      "\n",
      "*   **Step 1: Input:** You feed the AI 100,000 photos. Some are cats; some are not.\n",
      "*   **Step 2: Weighting:** Initially, the AI has no idea what a cat is. It looks at the pixels randomly. Each connection in its \"neural network\" has a **weight** (a number determining how important that feature is).\n",
      "*   **Step 3: Prediction:** The AI guesses: *\"I think this is a cat.\"* \n",
      "*   **Step 4: Error Correction (The \"Aha!\" Moment):** If the AI is wrong, a mathematical process called **backpropagation** adjusts the weights. It realizes, *\"Okay, pointy ears are more important for 'cat' than floppy ears.\"*\n",
      "*   **Step 5: Iteration:** It does this millions of times until it can identify a cat in a photo it has never seen before with high accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Different Types of AI\n",
      "Not all AI works the same way. Here are the three main flavors:\n",
      "\n",
      "*   **Machine Learning (ML):** The broad field of using statistics to find patterns. (e.g., Netflix suggesting a movie).\n",
      "*   **Deep Learning:** A subset of ML that uses many layers of neural networks to solve complex problems like facial recognition or voice translation.\n",
      "*   **Generative AI (like ChatGPT):** These are \"Large Language Models.\" They are trained on almost all the text on the internet. They don't \"know\" facts; instead, they calculate the **probability** of what word should come next in a sentence based on the context of the previous words.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Does AI \"Think\"?\n",
      "**No.** AI does not have a consciousness, opinions, or a soul. \n",
      "\n",
      "When you ask an AI a question, it isn't \"thinking\" about the answer. It is performing complex math to predict which sequence of words will best satisfy your request based on the patterns it learned during training. \n",
      "\n",
      "**Summary:** AI is basically **advanced statistics on steroids.** It turns the world into numbers, finds the patterns between those numbers, and uses those patterns to make predictions.\n"
     ]
    }
   ],
   "source": [
    "# Thinking \n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\", # Thinking supported in this model\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_level=\"low\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "687dd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 famous physicists and their key contributions:\n",
      "\n",
      "1.  **Sir Isaac Newton** (1642–1727)\n",
      "    *   **Key Contributions:**\n",
      "        *   **Laws of Motion:** Formulated the three fundamental laws of motion (inertia, F=ma, action-reaction) which are the basis of classical mechanics.\n",
      "        *   **Law of Universal Gravitation:** Described the force of attraction between any two objects with mass, explaining planetary orbits and phenomena like tides.\n",
      "        *   **Calculus:** Independently developed differential and integral calculus, a crucial mathematical tool for physics and engineering.\n",
      "        *   **Optics:** Pioneered work on the nature of light, demonstrating that white light is composed of a spectrum of colors.\n",
      "\n",
      "2.  **James Clerk Maxwell** (1831–1879)\n",
      "    *   **Key Contributions:**\n",
      "        *   **Maxwell's Equations:** A set of four partial differential equations that describe how electric and magnetic fields are generated and altered by each other and by charges and currents.\n",
      "        *   **Unification of Electromagnetism:** Unified electricity, magnetism, and light as manifestations of the same phenomenon – electromagnetic radiation.\n",
      "        *   **Prediction of Electromagnetic Waves:** Predicted the existence of electromagnetic waves (radio waves, microwaves, X-rays, etc.) and theorized that light itself is an electromagnetic wave.\n",
      "\n",
      "3.  **Albert Einstein** (1879–1955)\n",
      "    *   **Key Contributions:**\n",
      "        *   **Theory of Special Relativity:** Introduced the concepts of spacetime, mass-energy equivalence ($E=mc^2$), and the constancy of the speed of light for all observers.\n",
      "        *   **Theory of General Relativity:** Extended special relativity to include gravity, describing it as a curvature of spacetime caused by mass and energy. This provided a new understanding of gravity, black holes, and the universe's structure.\n",
      "        *   **Photoelectric Effect:** Explained the photoelectric effect (for which he won the Nobel Prize), demonstrating that light behaves as discrete packets of energy called photons, a cornerstone of quantum mechanics.\n",
      "        *   **Brownian Motion:** Provided a statistical explanation for Brownian motion, confirming the existence of atoms and molecules.\n",
      "Thoughts tokens: 838\n",
      "Output tokens: 473\n"
     ]
    }
   ],
   "source": [
    "# WIth thinking budget ( Backward compatible )\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",  # Thinking supported in this model\n",
    "    contents=\"Provide a list of 3 famous physicists and their key contributions\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=1024)\n",
    "        # Turn off thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        # Turn on dynamic thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=-1)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
    "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8bd88a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrow! *blinks slowly, then stretches out a paw for a gentle pat on the air* Hello yourself, human. Have you brought treats? Or perhaps a warm lap? Either is acceptable.\n"
     ]
    }
   ],
   "source": [
    "# Adding system instructions\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a cat. Your name is Neko.\"),\n",
    "    contents=\"Hello there\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97477fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I have two dogs in my house.\n",
      "Gemini: Oh, how wonderful! Having two dogs can be so much fun (and sometimes double the trouble, but always worth it!).\n",
      "\n",
      "What kind of dogs are they, and what are their names? I'd love to hear more about them!\n",
      "\n",
      "Is there anything specific you wanted to discuss about your dogs, or just sharing the good news?\n",
      "User: How many pets do I have?\n",
      "Gemini: Based on what you told me, you have **two** pets (your two dogs).\n",
      "\n",
      "Chat History:\n",
      "role - user: I have two dogs in my house.\n",
      "role - model: Oh, how wonderful! Having two dogs can be so much fun (and sometimes double the trouble, but always worth it!).\n",
      "\n",
      "What kind of dogs are they, and what are their names? I'd love to hear more about them!\n",
      "\n",
      "Is there anything specific you wanted to discuss about your dogs, or just sharing the good news?\n",
      "role - user: How many pets do I have?\n",
      "role - model: Based on what you told me, you have **two** pets (your two dogs).\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversations\n",
    "conversation = [\n",
    "    \"I have two dogs in my house.\",\n",
    "    \"How many pets do I have?\",\n",
    "]\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "for message in conversation:\n",
    "    response = chat.send_message(message)\n",
    "    print(\"User:\", message)\n",
    "    print(\"Gemini:\", response.text)\n",
    "\n",
    "\n",
    "print(\"\\nChat History:\")\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f6e5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence (AI) is a broad field of computer science dedicated to creating machines that can perform tasks that typically require human intelligence. This includes things-> like learning, problem-solving, decision-making, understanding language, and recognizing patterns.\n",
      "\n",
      "At its core, AI works by **learning from data** and then **applying that learning to new situations** to make predictions, classifications, or decisions.\n",
      "\n",
      "Here's a breakdown of how AI generally works:\n",
      "\n",
      "###-> 1. The Foundation: Data and Algorithms\n",
      "\n",
      "*   **Data is the Fuel:** AI systems are incredibly data-hungry. They need vast amounts of information – text, images, numbers, audio, etc. – to learn from. The quality and quantity of this data directly impact the AI's performance. Think-> of it like a student needing textbooks and practice problems.\n",
      "*   **Algorithms are the Recipes:** These are sets of rules and instructions that tell the AI system how to process the data, identify patterns, and make decisions. Different algorithms are suited for different tasks (e.g., one for classifying images, another for predicting-> stock prices).\n",
      "\n",
      "### 2. The Learning Process (Machine Learning)\n",
      "\n",
      "Most modern AI systems learn through a process called **Machine Learning (ML)**, which is a subset of AI. Instead of being explicitly programmed for every possible scenario, ML models learn from data.\n",
      "\n",
      "The learning process typically involves two main phases:\n",
      "\n",
      "####-> a. Training Phase\n",
      "\n",
      "1.  **Feeding the Data:** The AI model is fed a large dataset.\n",
      "    *   **Supervised Learning:** This is the most common type. The data is \"labeled,\" meaning it includes both the input and the correct output. For example, you might show an AI millions-> of pictures of cats and dogs, with each picture clearly labeled \"cat\" or \"dog.\"\n",
      "    *   **Unsupervised Learning:** The data is *unlabeled*. The AI tries to find hidden patterns, structures, or groupings within the data on its own. For example, clustering similar customers together without being told what-> constitutes a \"customer segment.\"\n",
      "    *   **Reinforcement Learning:** The AI learns by trial and error through interaction with an environment. It receives \"rewards\" for desired actions and \"penalties\" for undesired ones, iteratively improving its strategy (e.g., an AI learning to play chess by being rewarded for wins).\n",
      "\n",
      "2->.  **Pattern Recognition:** The algorithm processes this data, looking for relationships, correlations, and distinguishing features. For instance, in the cat/dog example, it might learn that cats often have pointed ears and a certain type of whiskers, while dogs have different ear shapes and muzzle structures.\n",
      "\n",
      "3.  **Adjusting Parameters (->Weights and Biases):** The algorithm has internal parameters (often called \"weights\" and \"biases\" in neural networks). During training, these parameters are iteratively adjusted to minimize the difference between the AI's predicted output and the actual correct output (in supervised learning). This is like fine-tuning a complex equation-> to fit the data as perfectly as possible.\n",
      "\n",
      "4.  **Model Creation:** The end result of the training phase is a \"trained model\" – a mathematical representation that has learned patterns and can now make predictions or decisions based on new, unseen data.\n",
      "\n",
      "#### b. Inference/Prediction Phase\n",
      "\n",
      "1.  **New-> Data Input:** Once trained, the model is exposed to new, unseen data (e.g., a new picture of an animal it hasn't seen before).\n",
      "2.  **Applying Learned Patterns:** The model uses the patterns and relationships it learned during training to analyze this new data.\n",
      "3.  **Output->/Decision:** It then generates an output – a prediction, a classification, a recommendation, or an action. For our animal example, it would classify the new image as \"cat\" or \"dog\" with a certain probability.\n",
      "\n",
      "### 3. Key AI Techniques\n",
      "\n",
      "While Machine Learning describes the learning process, specific techniques-> are used within it:\n",
      "\n",
      "*   **Neural Networks & Deep Learning:** Inspired by the human brain, these are complex networks of interconnected \"neurons\" (mathematical functions) arranged in layers. \"Deep learning\" refers to neural networks with many layers (deep networks). They are exceptionally good at finding intricate patterns in large,-> unstructured data like images, audio, and natural language.\n",
      "*   **Natural Language Processing (NLP):** Enables AI to understand, interpret, and generate human language. Powers chatbots, translation tools, and spam filters.\n",
      "*   **Computer Vision:** Allows AI to \"see\" and interpret visual information from images and videos. Used in-> facial recognition, self-driving cars, and medical image analysis.\n",
      "*   **Expert Systems:** Older AI approach that uses a set of \"if-then\" rules created by human experts to solve problems within a specific domain.\n",
      "*   **Robotics:** Integrates AI with physical machines to enable them to perceive-> their environment, plan actions, and execute tasks.\n",
      "\n",
      "### 4. How it \"Thinks\" (It Doesn't, Not Really)\n",
      "\n",
      "It's important to understand that AI doesn't \"think\" or \"understand\" in the human sense. It operates based on **mathematical computations, statistical probabilities->, and pattern matching**. When an AI identifies a cat, it's not because it understands what a cat *is*, but because it has learned to associate specific pixel patterns, shapes, and textures with the label \"cat\" based on its training data.\n",
      "\n",
      "### 5. Iteration and Improvement\n",
      "\n",
      "AI development is an iterative-> process. Models are continually refined, retrained with new data, and evaluated to improve their accuracy and robustness.\n",
      "\n",
      "In essence, AI works by enabling computers to learn from experience, identify patterns in data, and then use those learned patterns to make intelligent decisions or perform tasks without being explicitly programmed for every single step.->"
     ]
    }
   ],
   "source": [
    "# Streaming responses\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\"Explain how AI works\"]\n",
    ")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"->\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6b37d",
   "metadata": {},
   "source": [
    "# Thought Signatures\n",
    "\n",
    "Thought signatures are encrypted representations of the model's internal thought process and are used to preserve reasoning context across multi-step interactions. When using thinking models (such as the Gemini 3 and 2.5 series), the API may return a thoughtSignature field within the content parts of the response (e.g., text or functionCall parts).\n",
    "\n",
    "As a general rule, if you receive a thought signature in a model response, you should pass it back exactly as received when sending the conversation history in the next turn. When using Gemini 3 models, you must pass back thought signatures during function calling, otherwise you will get a validation error (4xx status code). This includes when using the minimal thinking level setting for Gemini 3 Flash.\n",
    "\n",
    "Note: If you use the official Google Gen AI SDKs and use the chat feature (or append the full model response object directly to history), thought signatures are handled automatically. You do not need to manually extract or manage them, or change your code.\n",
    "\n",
    "Gemini 3 returns thought signatures for all model responses (responses from the API) with a function call. Thought signatures show up in the following cases:\n",
    "\n",
    "When there are parallel function calls, the first function call part returned by the model response will have a thought signature.\n",
    "When there are sequential function calls (multi-step), each function call will have a signature and you must pass all signatures back.\n",
    "Model responses without a function call will return a thought signature inside the last part returned by the model.\n",
    "\n",
    "\n",
    "(https://ai.google.dev/gemini-api/docs/thought-signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f984",
   "metadata": {},
   "source": [
    "# Structured Data Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0ad9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_name='Chocolate Chip Cookies' prep_time_minutes=None ingredients=[Ingredient(name='all-purpose flour', quantity='2 and 1/4 cups'), Ingredient(name='baking soda', quantity='1 teaspoon'), Ingredient(name='salt', quantity='1 teaspoon'), Ingredient(name='unsalted butter (softened)', quantity='1 cup'), Ingredient(name='granulated sugar', quantity='3/4 cup'), Ingredient(name='packed brown sugar', quantity='3/4 cup'), Ingredient(name='vanilla extract', quantity='1 teaspoon'), Ingredient(name='large eggs', quantity='2'), Ingredient(name='semisweet chocolate chips', quantity='2 cups')] instructions=['Preheat the oven to 375°F (190°C).', 'In a small bowl, whisk together the flour, baking soda, and salt.', 'In a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.', 'Beat in the vanilla and eggs, one at a time.', 'Gradually beat in the dry ingredients until just combined.', 'Stir in the chocolate chips.', 'Drop by rounded tablespoons onto ungreased baking sheets and bake for 9 to 11 minutes.']\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Ingredient(BaseModel):\n",
    "    name: str = Field(description=\"Name of the ingredient.\")\n",
    "    quantity: str = Field(description=\"Quantity of the ingredient, including units.\")\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    recipe_name: str = Field(description=\"The name of the recipe.\")\n",
    "    prep_time_minutes: Optional[int] = Field(description=\"Optional time in minutes to prepare the recipe.\")\n",
    "    ingredients: List[Ingredient]\n",
    "    instructions: List[str]\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Please extract the recipe from the following text.\n",
    "The user wants to make delicious chocolate chip cookies.\n",
    "They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,\n",
    "1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,\n",
    "3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.\n",
    "For the best part, they'll need 2 cups of semisweet chocolate chips.\n",
    "First, preheat the oven to 375°F (190°C). Then, in a small bowl, whisk together the flour,\n",
    "baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar\n",
    "until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry\n",
    "ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons\n",
    "onto ungreased baking sheets and bake for 9 to 11 minutes.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": Recipe.model_json_schema(),\n",
    "    },\n",
    ")\n",
    "\n",
    "recipe = Recipe.model_validate_json(response.text)\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec34f1",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b076f",
   "metadata": {},
   "source": [
    "### From Function Declaration\n",
    "```\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf9b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to call: create_bar_chart\n",
      "Arguments: {'labels': ['Q1', 'Q2', 'Q3'], 'values': [50000, 75000, 60000], 'title': 'Quarterly Sales'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Define the function declaration for the model\n",
    "create_chart_function = {\n",
    "    \"name\": \"create_bar_chart\",\n",
    "    \"description\": \"Creates a bar chart given a title, labels, and corresponding values.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The title for the chart.\",\n",
    "            },\n",
    "            \"labels\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"description\": \"List of labels for the data points (e.g., ['Q1', 'Q2', 'Q3']).\",\n",
    "            },\n",
    "            \"values\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"number\"},\n",
    "                \"description\": \"List of numerical values corresponding to the labels (e.g., [50000, 75000, 60000]).\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"title\", \"labels\", \"values\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Create a bar chart titled 'Quarterly Sales' with data: Q1: 50000, Q2: 75000, Q3: 60000.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Check for a function call\n",
    "if response.candidates[0].content.parts[0].function_call:\n",
    "    function_call = response.candidates[0].content.parts[0].function_call\n",
    "    print(f\"Function to call: {function_call.name}\")\n",
    "    print(f\"Arguments: {function_call.args}\")\n",
    "    #  In a real app, you would call your function here using a charting library:\n",
    "    #  result = create_bar_chart(**function_call.args)\n",
    "else:\n",
    "    print(\"No function call found in the response.\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e558323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-0.5694650173187256,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            ),\n",
       "            thought_signature=b'\\n\\xff\\x02\\x01\\x8f=k_\\x1e\\xc0\\x90\\x8ePdKg\\x1b7\\xd3w\\x1e\"\\xdb\\x8aI\\xf9B/$\\xd7\\xc6V\\x7f`\\xbb\\x9d\\xcf.\\xef~\\x88o5\\x08\\x93\\'C\\xda\\x94\\xb6\\x13\\xa3\\x91^\\xbb\\xef\\xfc\\x86\\xff\\xa1\\x9e4\\xd3\\xf9\\xcc\\x85k\\x1a\\xe2\\xe4\\x8cd\\x08\\xaaB\\x0b\\tn\\x02\\x1cK\\x04\\xeb\\xf3\\x18k\\xb2\\x13|l\\xe7\\xfc\\x1c\\x07*Ig...'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  create_time=datetime.datetime(2026, 1, 22, 3, 42, 18, 34381, tzinfo=TzInfo(0)),\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='mpxxac2MAq-Hz7sP3aj5uAU',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=10>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=20,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=20\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=138,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=138\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=112,\n",
       "    total_token_count=270,\n",
       "    traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66892230",
   "metadata": {},
   "source": [
    "## Function Execution ( The mannual way )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b081aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Step 1: Model Suggests Function Call ==================\n",
      "id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values' partial_args=None will_continue=None\n",
      "================== Step 2: Function Execution Result ==================\n",
      "Function execution result: {'brightness': 25, 'colorTemperature': 'warm'}\n",
      "================== Step 4: Wrap Function Response ==================\n",
      "media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 25,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 4a: Updated Contents ==================\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='Turn the lights down to a romantic level' thought=None thought_signature=None video_metadata=None\n",
      "Role: model, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=FunctionCall(\n",
      "  args={\n",
      "    'brightness': 25,\n",
      "    'color_temp': 'warm'\n",
      "  },\n",
      "  name='set_light_values'\n",
      ") function_response=None inline_data=None text=None thought=None thought_signature=b'\\n\\x8c\\x05\\x01\\x8f=k_\\t\\x8e8\\xd8M\\x10\\xc4\\x9eh\\x8f+#\\xb6r\\x88p\\xe2\\x18\\x85\\xeb!\\x0b\\x99=\\xb8\\x99\\x0e\\rPi\\xdc\\xac,\\xbe!\\x1c\\xaa&7E&\\xd7/7*\\xfaC\\xc6\\xf3C\\x02\\xfc\\xcb\\xfdU\\x15\\x91\\xfc\\x9fS+\\x07d\\xd7\\xe6\\xff\\xdf]&f\\xbf\\xc1d\\x0b\\x82\\xc4?|\\x19\\x03;\\xa3\\\\h\\x7f\\x1d\\x1b\\xc9+p?5mv\\xf8v7\\x02\\xb5\\x9e\\xbf\\x97\\x90B\\x9eClw\\xb8\\xe5\\xc9?\\xa7\\xf0\\x9a.\\xc3\\x16\\r\\xa9\\xa15t\\xb8[k\\xf6z\\xe7\\xa0\\x93\\x12\\x15\\xdc($\\xafz;G\\xea>\\xa1j\\xe4\\xa0\\xa9\\xec\\x83\\xb7\\xc5)\\xe9Z\\xf7\\xe9j\\xcd\\x96\\xc3\\x98\\xd7\\xb1\\x12{\\x01\\xf7\\x8a\\x88Z\\x99\\xbe7\\xcb\\xddT\\x92\\xfd\\xd9\\xde\\xd4\\x8d\\x92\\x84|\\xfd\\x94\\xf9\\x04\\xd8\\xd7\\x0bu\"HU\\xf5i\\xed_H\\x82\\x0br\\x16\\xa6\\xb8\\x90\\x81\\x0e\\xab\\xe7]\\xd8x\\xe2r\\xae\\x0c\\x15K\\xc3\\xdc\\xc8R\\xb2\\xa3s\\xa8-\\xc8<\\xdf\\xac\\x08\\x7f\\xab#F\\x9c\\x13X\\x1d1\\xda\\x85\\x13\\x0f\\xc4\\xd1\\xf5Eq\\xb4Y\\xc6\\xe6.j\\xbb3\\xa6\\x8e\\x91\\xa5Y\\xd7\\'\\xc0\\x82?\\x97\\xebA\\xd4i\\nrg\\xe2\\xe1\\xd7\\x08AF\\x0e\\x08k\\xe8,\\x9e\\xa9~\\xb7\\x89\\xbf\\x99{\\xef\\xc7@z\\xc3[w`\\x01\\xa3\\xf8\\xc0\\x7f\\xa1\\xc8c#j\\x96\\xe5\\x07\\x87N\\xa7\\xceO\\xba\\xfaa\\xbe\\xade\\x04\\xe4p\\x10\\xd4t\\xda\\xa1\\x1b\\xf43\\xa9\\x84\\xb0bJ\\x92\"\\x18\\xa4\\x04)fX\\n<\\xd1+\\x0b\\xd5\\xf9\\x18c\"]e\\xfa<_{\\xcc\\x1d\\x8dJM\\xf1e}q\\xc6\\xf8m{f9\\xb4\\xf0\\xae\\x0e\\xf6]^\\x83O\\xea\\x05\\x92Y\\x1d\\xf0<\\x95\\xbcZV\\x07\\xe7\\xef\\xca\\xf1\\xb9\\xb4\\x04\\x0c:c\\x9c\\x8d\\x85\\x87\\xf3\\xba\\xa7\\xc7\\x91\\x9f\\x9a\\x95r\\xe7\\xf0L\\xb3\\x9fT7K\\xc3Fg)[\\x14\\x13-\\x83n\\xf1\\xb2O(\\x0e\\xcc\\xe5,\\x14\\xd9\\x12\\xd0\\xee\\xd7o\\xe8\\xd2\\x9d\\xe6\\n\\t~\\xb7\\x91\\xfa\\x0fN&\\xdeGR\\xa3\\x18\\xb7^\\xd3\\x17\\xd4\\x87\\xfb\\x8c\\xc4\\x12\\x81A\\xf0b\\xc1\\xde\\x1e\\t\\xc5\\xdc.\\x93j\\xd7\\x8d\\xb3\\x89\\xfb\\xe3D.I\\xf7\\xfd\\xe1\\xccFY\\xf5\\x17\\xe6\\'\\xc0Y\\xf1\\x1d\\x9c\\x96Vp\\x0e\\x86%\\xfc\\xce{\\x871e\\x0b\\x9d\\x87\\x84F\\x1a\\'P\\x08`\\xd7V\\t7x\\xdb \\xdd\\xd2\\xa9&\\xcac-\\x90\\xc5\\xacw\\x83\\x1cR\\x04S\\xec$\\x19f\\xdfS\\x95\\x0c\\xca\\x8c\\x7f\\xe84\\xb3\\xa4\\xa8\\xce@\\x14P\\xa4\\x96\\x8c=\\x0f\\xef\\x0f\\xab\\xd8\\x80\\xa1\\xa8d,>s\\x80\\xeb\\xf0\\xf2\\xc9\\xeee7\\x8a\\xd3\\xdaG\\xe6V\\x9f\\xe9\\x1dSt\\x1b\\x8f\\xf8\\xb1\\x051<t;_\\xd2\\xca9\\xc1\\xf31HZ\"H' video_metadata=None\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 25,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 5: Final Model Response ==================\n",
      "Done. The lights are now set to a romantic level.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# Define a function that the model can call to control smart lights\n",
    "set_light_values_declaration = {\n",
    "    \"name\": \"set_light_values\",\n",
    "    \"description\": \"Sets the brightness and color temperature of a light.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n",
    "            },\n",
    "            \"color_temp\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n",
    "                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"brightness\", \"color_temp\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# This is the actual function that would be called based on the model's suggestion\n",
    "def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
    "\n",
    "    Args:\n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "tools = types.Tool(function_declarations=[set_light_values_declaration])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Define user prompt\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"================== Step 1: Model Suggests Function Call ==================\")\n",
    "print(response.candidates[0].content.parts[0].function_call)\n",
    "\n",
    "\n",
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"set_light_values\":\n",
    "    result = set_light_values(**tool_call.args)\n",
    "    print(\"================== Step 2: Function Execution Result ==================\")\n",
    "    print(f\"Function execution result: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a function response part\n",
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "print(\"================== Step 4: Wrap Function Response ==================\")\n",
    "print(function_response_part)\n",
    "\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "print(\"================== Step 4a: Updated Contents ==================\")\n",
    "for content in contents:\n",
    "    for part in content.parts:\n",
    "        print(f'Role: {content.role}, Part: {part}')\n",
    "\n",
    "client = genai.Client()\n",
    "final_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=config,\n",
    "    contents=contents,\n",
    ")\n",
    "\n",
    "print(\"================== Step 5: Final Model Response ==================\")\n",
    "print(final_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e5cbf",
   "metadata": {},
   "source": [
    "## Parallel function calling\n",
    "\n",
    "In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5cd5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_disco_ball = {\n",
    "    \"name\": \"power_disco_ball\",\n",
    "    \"description\": \"Powers the spinning disco ball.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"power\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether to turn the disco ball on or off.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"power\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "start_music = {\n",
    "    \"name\": \"start_music\",\n",
    "    \"description\": \"Play some music matching the specified parameters.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"energetic\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is energetic or not.\",\n",
    "            },\n",
    "            \"loud\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is loud or not.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"energetic\", \"loud\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "dim_lights = {\n",
    "    \"name\": \"dim_lights\",\n",
    "    \"description\": \"Dim the lights.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"brightness\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7f87055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Forced function calling\n",
      "power_disco_ball(power=True)\n",
      "start_music(loud=True, energetic=True)\n",
      "dim_lights(brightness=0.3)\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "house_tools = [\n",
    "    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n",
    "]\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=house_tools,\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
    "        disable=True\n",
    "    ),\n",
    "    # Force the model to call 'any' function, instead of chatting.\n",
    "    tool_config=types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\", config=config)\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call\n",
    "print(\"Example 1: Forced function calling\")\n",
    "for fn in response.function_calls:\n",
    "    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "    print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb78cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-1.0244757652282714,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            ),\n",
       "            thought_signature=b'\\n\\xba\\x03\\x01\\x8f=k_\\xd9\\xde\\xf6\\x0c4>J\\x88}e-\\xd0g\\n\\xd1\\xba\\t\\x8aX\\xa2\\t\\xff\\x17B\\\\WxO\\xc3=O\\x81\\x11]/\\xf6\\xd3\\x83\\xe4\\x7fZ\\xe4C\\x03Z\\xcc\\xa6\\x9dH2\\x81\\xdaM\\xcd\\xd8\\xcf\\xfe\\xb7\\xbd\\xefC3 \\x95\\t\\x81\\x92\\x93(_VUS\\xceuM\\x1b\\xfd\\x1eK\\xe5\\xfa\\x7f\\xa7\\xf3\\\\\\xf3\\xdf\\x18...'\n",
       "          ),\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            )\n",
       "          ),\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            )\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  create_time=datetime.datetime(2026, 1, 22, 4, 19, 3, 198572, tzinfo=TzInfo(0)),\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='N6VxaayPDO6bseMPy4eRqAo',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=10>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=20,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=20\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=99,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=99\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=121,\n",
       "    total_token_count=240,\n",
       "    traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1afa08",
   "metadata": {},
   "source": [
    "## Automatic Function Calls\n",
    "The Python SDK supports automatic function calling, which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.\n",
    "\n",
    "In the mannual way we were doing\n",
    "\n",
    "```\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "```\n",
    "\n",
    "Now with automatic function calling we can do\n",
    "\n",
    "```\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdb009ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: power_disco_ball_impl\n",
      "Function called: start_music_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, the party is starting! The disco ball is spinning, energetic and loud music is playing, and the lights have been dimmed to 30%. Let's get this party going!\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "power_disco_ball_state = None\n",
    "start_music_state = None\n",
    "dim_lights_state = None\n",
    "\n",
    "\n",
    "# Actual function implementations\n",
    "def power_disco_ball_impl(power: bool) -> dict:\n",
    "    \"\"\"Powers the spinning disco ball.\n",
    "\n",
    "    Args:\n",
    "        power: Whether to turn the disco ball on or off.\n",
    "\n",
    "    Returns:\n",
    "        A status dictionary indicating the current state.\n",
    "    \"\"\"\n",
    "    print(\"Function called: power_disco_ball_impl\")\n",
    "    global power_disco_ball_state\n",
    "    power_disco_ball_state =  {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "    return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "\n",
    "def start_music_impl(energetic: bool, loud: bool) -> dict:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        energetic: Whether the music is energetic or not.\n",
    "        loud: Whether the music is loud or not.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the music settings.\n",
    "    \"\"\"\n",
    "    print(\"Function called: start_music_impl\")\n",
    "    music_type = \"energetic\" if energetic else \"chill\"\n",
    "    volume = \"loud\" if loud else \"quiet\"\n",
    "    global start_music_state\n",
    "    start_music_state = {\"music_type\": music_type, \"volume\": volume}\n",
    "    return {\"music_type\": music_type, \"volume\": volume}\n",
    "\n",
    "def dim_lights_impl(brightness: float) -> dict:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the new brightness setting.\n",
    "    \"\"\"\n",
    "    print(\"Function called: dim_lights_impl\")\n",
    "    global dim_lights_state\n",
    "    dim_lights_state = {\"brightness\": brightness}\n",
    "    return {\"brightness\": brightness}\n",
    "\n",
    "# Configure the client\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party!\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9f5f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'status': 'Disco ball powered on'},\n",
       " {'music_type': 'energetic', 'volume': 'loud'},\n",
       " {'brightness': 0.3})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_disco_ball_state, start_music_state, dim_lights_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb7164",
   "metadata": {},
   "source": [
    "#### Trying to manipulate the order of Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6fa288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Party started! The music is loud and energetic, the disco ball is spinning, and the lights are at 50% brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8e20df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, I've set up the party: energetic and loud music, disco ball on, and lights at 50%.\n",
      "\n",
      "Then, I've wound things down: chill and quiet music, disco ball off, and lights at full brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness. Then again start the music quietly and chill, turn off the disco ball, and set the lights to full brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e919e35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'status': 'Disco ball powered off'},\n",
       " {'music_type': 'chill', 'volume': 'quiet'},\n",
       " {'brightness': 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_disco_ball_state, start_music_state, dim_lights_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d6f11",
   "metadata": {},
   "source": [
    "## Compositional function calling\n",
    "\n",
    "Passing the output of one function as the input to another function. This is useful when the functions are dependent on each other and the output of one function is needed as input for another function. For example, you can first retrieve user information and then use that information to generate a personalized recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "287385a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather_forecast(location=London)\n",
      "Tool Response: {'temperature': 25, 'unit': 'celsius'}\n",
      "Tool Call: set_thermostat_temperature(temperature=20)\n",
      "Tool Response: {'status': 'success'}\n",
      "The thermostat has been set to 20°C.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Example Functions\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n",
    "    print(f\"Tool Call: get_weather_forecast(location={location})\")\n",
    "    # TODO: Make API call\n",
    "    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n",
    "    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n",
    "    # TODO: Interact with a thermostat API\n",
    "    print(\"Tool Response: {'status': 'success'}\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "# Configure the client and model\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the final, user-facing response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ced19",
   "metadata": {},
   "source": [
    "## Final Notes for Function Calling\n",
    "\n",
    "### Modes\n",
    "The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.function_calling_config.\n",
    "\n",
    "-AUTO (Default): The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n",
    "- ANY: The model is constrained to always predict a function call and guarantees function schema adherence. If allowed_function_names is not specified, the model can choose from any of the provided function declarations. If allowed_function_names is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).\n",
    "- NONE: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n",
    "- VALIDATED (Preview): The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```\n",
    "# Configure function calling mode\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config=types.FunctionCallingConfig(\n",
    "        mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the generation config\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],  # not defined here.\n",
    "    tool_config=tool_config,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### Combine native tools with function calling\n",
    "\n",
    "Multi-tool use is a-Live API only feature at the moment. The run() function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n",
    "\n",
    "```\n",
    "# Multiple tasks example - combining lights, code execution, and search\n",
    "prompt = \"\"\"\n",
    "  Hey, I need you to do three things for me.\n",
    "\n",
    "    1.  Turn on the lights.\n",
    "    2.  Then compute the largest prime palindrome under 100000.\n",
    "    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n",
    "\n",
    "  Thanks!\n",
    "  \"\"\"\n",
    "\n",
    "tools = [\n",
    "    {'google_search': {}},\n",
    "    {'code_execution': {}},\n",
    "    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n",
    "]\n",
    "\n",
    "# Execute the prompt with specified tools in audio modality\n",
    "await run(prompt, tools=tools, modality=\"AUDIO\")\n",
    "```\n",
    "\n",
    "### Live API: https://ai.google.dev/gemini-api/docs/live?example=mic-stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df36896",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API provides a code execution tool that enables the model to generate and run Python code. The model can then learn iteratively from the code execution results until it arrives at a final output. You can use code execution to build applications that benefit from code-based reasoning. For example, you can use code execution to solve equations or process text. You can also use the libraries included in the code execution environment to perform more specialized tasks.\n",
    "\n",
    "Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution\n",
    "\n",
    "- Supported libraries\n",
    "The code execution environment includes the following libraries:\n",
    "\n",
    "attrs\n",
    "chess\n",
    "contourpy\n",
    "fpdf\n",
    "geopandas\n",
    "imageio\n",
    "jinja2\n",
    "joblib\n",
    "jsonschema\n",
    "jsonschema-specifications\n",
    "lxml\n",
    "matplotlib\n",
    "mpmath\n",
    "numpy\n",
    "opencv-python\n",
    "openpyxl\n",
    "packaging\n",
    "pandas\n",
    "pillow\n",
    "protobuf\n",
    "pylatex\n",
    "pyparsing\n",
    "PyPDF2\n",
    "python-dateutil\n",
    "python-docx\n",
    "python-pptx\n",
    "reportlab\n",
    "scikit-learn\n",
    "scipy\n",
    "seaborn\n",
    "six\n",
    "striprtf\n",
    "sympy\n",
    "tabulate\n",
    "tensorflow\n",
    "toolz\n",
    "xlrd\n",
    "\n",
    "###### You can't install your own libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7ee5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready! Go ahead and share it with me. Whether it's algebra, calculus, geometry, statistics, or anything else, I'll do my best to help you solve it or explain the concepts involved.\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "primes = []\n",
      "num = 2\n",
      "while len(primes) < 50:\n",
      "    if is_prime(num):\n",
      "        primes.append(num)\n",
      "    num += 1\n",
      "\n",
      "print(f\"The first 50 primes are: {primes}\")\n",
      "print(f\"The number of primes found: {len(primes)}\")\n",
      "print(f\"The sum of the first 50 primes is: {sum(primes)}\")\n",
      "The first 50 primes are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\n",
      "The number of primes found: 50\n",
      "The sum of the first 50 primes is: 5117\n",
      "\n",
      "The sum of the first 50 prime numbers is **5,117**.\n",
      "\n",
      "To find this, I used a script to identify the primes starting from 2 and stopped once the 50th prime (which is 229) was reached. \n",
      "\n",
      "**The first 50 prime numbers are:**\n",
      "2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229.\n",
      "\n",
      "**Calculation:**\n",
      "$2 + 3 + 5 + \\dots + 229 = 5,117$\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat.send_message(\"I have a math question for you.\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"What is the sum of the first 50 prime numbers? \"\n",
    "    \"Generate and run code for the calculation, and make sure you get all 50.\"\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "        print(part.text)\n",
    "    if part.executable_code is not None:\n",
    "        print(part.executable_code.code)\n",
    "    if part.code_execution_result is not None:\n",
    "        print(part.code_execution_result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cc021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htll-ir-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
