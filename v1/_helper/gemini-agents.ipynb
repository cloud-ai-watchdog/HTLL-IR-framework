{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33604932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "env_path = f\"{current_dir}/.env\"\n",
    "load_dotenv(dotenv_path=env_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2392bf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY loaded successfully.\n",
      "GOOGLE_APPLICATION_CREDENTIALS loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if gemini API key is loaded\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key:\n",
    "    print(\"GEMINI_API_KEY loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GEMINI_API_KEY.\")\n",
    "\n",
    "# Check for GOOGLE_APPLICATION_CREDENTIALS\n",
    "if os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"):\n",
    "    print(\"GOOGLE_APPLICATION_CREDENTIALS loaded successfully.\")\n",
    "else:\n",
    "    print(\"Failed to load GOOGLE_APPLICATION_CREDENTIALS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8b372f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following this docs: https://ai.google.dev/gemini-api/docs/quickstart\n",
    "# Pricing: https://ai.google.dev/gemini-api/docs/pricing\n",
    "# Avaialable models: https://ai.google.dev/gemini-api/docs/models\n",
    "# Writing good prompts: https://ai.google.dev/gemini-api/docs/prompting-strategies\n",
    "# Langchain integration: https://ai.google.dev/gemini-api/docs/langgraph-example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efd2fe",
   "metadata": {},
   "source": [
    "## Some Basic Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dc428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv add google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"]=\"true\"\n",
    "from google import genai\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f72a1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options, pick the one that best suits your need for brevity:\n",
      "\n",
      "*   **Option 1 (Mechanism-focused):** AI learns patterns from data to make decisions or predictions.\n",
      "*   **Option 2 (Human-like focus):** AI learns from data to mimic human-like intelligence and solve problems.\n",
      "*   **Option 3 (Very short):** AI learns from data to make smart decisions.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17921e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To understand how AI works, it helps to stop thinking of it as a \"robot brain\" and start thinking of it as **extremely advanced pattern recognition.**\n",
      "\n",
      "While traditional software follows a strict list of \"if-then\" rules written by humans, modern AI **learns** how to complete a task by analyzing massive amounts of data.\n",
      "\n",
      "Here is the breakdown of how that process actually works:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Foundation: Data\n",
      "AI needs \"fuel\" to learn, and that fuel is data. To teach an AI to recognize a cat, you don't give it a definition of a cat (ears, fur, whiskers). Instead, you feed it millions of images labeled \"cat\" and millions labeled \"not cat.\"\n",
      "\n",
      "### 2. The Engine: Machine Learning\n",
      "**Machine Learning (ML)** is the most common type of AI today. It uses mathematical algorithms to find patterns in that data.\n",
      "*   **The Learning Process:** The AI looks at the data and makes a guess. \n",
      "*   **The Correction:** If the guess is wrong, the system adjusts its internal mathematical formulas (called \"weights\") to be more accurate next time.\n",
      "*   **The Result:** After repeating this millions of times, the AI becomes incredibly good at identifying the underlying patterns that define a \"cat.\"\n",
      "\n",
      "### 3. The Structure: Neural Networks\n",
      "Most high-level AI (like ChatGPT or Midjourney) uses **Neural Networks**. These are inspired by the human brain.\n",
      "*   They consist of layers of \"neurons\" (code-based nodes). \n",
      "*   Each layer looks for something different. In an image, the first layer might look for simple lines, the next for shapes (circles, triangles), and the final layers for complex features (eyes, ears).\n",
      "*   When many layers are stacked together, it is called **Deep Learning.**\n",
      "\n",
      "### 4. How Generative AI (like ChatGPT) Works\n",
      "Generative AI doesn't actually \"know\" facts. It is a **prediction engine.**\n",
      "*   When you ask ChatGPT a question, it isn't \"thinking.\" It is calculating the **statistical probability** of which word should come next. \n",
      "*   If you type \"The cat sat on the...\", the AI’s training tells it there is an 80% chance the next word is \"mat\" and a 1% chance it is \"refrigerator.\" It chooses the most likely path based on the billions of sentences it has read.\n",
      "\n",
      "### 5. Training vs. Inference\n",
      "There are two main stages in an AI's life:\n",
      "1.  **Training:** This is the expensive, \"school\" phase where the AI processes data and learns patterns. This takes months and massive amounts of electricity.\n",
      "2.  **Inference:** This is the \"test\" phase. This is when you use the AI. It uses its already-learned patterns to answer your prompt.\n",
      "\n",
      "### Summary: An Analogy\n",
      "*   **Traditional Coding** is like a **Recipe**: \"Add two eggs, stir for five minutes, bake at 350 degrees.\" If you follow the steps exactly, you get the same result every time.\n",
      "*   **AI** is like **a Chef who has tasted 10 million meals**: You don't give it a recipe; you just show it the finished dish. By tasting so many meals, the chef eventually figures out exactly which ingredients and temperatures are needed to recreate any flavor you ask for.\n",
      "\n",
      "### Why is this important?\n",
      "Because AI works on patterns and probability rather than \"logic,\" it can sometimes **hallucinate** (make mistakes). It isn't checking a database for truth; it is simply predicting what \"looks\" or \"sounds\" right based on its training.\n"
     ]
    }
   ],
   "source": [
    "# Thinking \n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-3-flash-preview\", # Thinking supported in this model\n",
    "    contents=\"How does AI work?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_level=\"low\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "687dd6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 famous physicists and their key contributions:\n",
      "\n",
      "1.  **Sir Isaac Newton (1642 – 1727)**\n",
      "    *   **Key Contributions:** Formulated the **laws of motion** and the **law of universal gravitation**, which laid the foundation for classical mechanics. He also made significant contributions to optics (developing the reflecting telescope and a theory of color) and developed calculus (independently of Gottfried Leibniz). His work described the universe as a predictable, mechanical system.\n",
      "\n",
      "2.  **Albert Einstein (1879 – 1955)**\n",
      "    *   **Key Contributions:** Developed the **theories of special and general relativity**, which fundamentally changed our understanding of space, time, gravity, and the universe. His famous equation, **E=mc²**, established the equivalence of mass and energy. He also explained the **photoelectric effect**, contributing significantly to quantum theory and earning him the Nobel Prize in Physics in 1921.\n",
      "\n",
      "3.  **Marie Curie (1867 – 1934)**\n",
      "    *   **Key Contributions:** A pioneering researcher in **radioactivity** (a term she coined). She discovered the elements **polonium** and **radium**, and developed techniques for isolating radioactive isotopes. She was the first woman to win a Nobel Prize, the first person and only woman to win Nobel Prizes in two different scientific fields (Physics in 1903, Chemistry in 1911), and the only person to win Nobel Prizes in multiple sciences.\n",
      "Thoughts tokens: 865\n",
      "Output tokens: 322\n"
     ]
    }
   ],
   "source": [
    "# WIth thinking budget ( Backward compatible )\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",  # Thinking supported in this model\n",
    "    contents=\"Provide a list of 3 famous physicists and their key contributions\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=1024)\n",
    "        # Turn off thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        # Turn on dynamic thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=-1)\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
    "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8bd88a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrow. *blinks slowly, tail gives a lazy twitch* Oh, it's you. Hello there. Did you bring treats? Or perhaps a nice sunbeam to bask in?\n"
     ]
    }
   ],
   "source": [
    "# Adding system instructions\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a cat. Your name is Neko.\"),\n",
    "    contents=\"Hello there\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "97477fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I have two dogs in my house.\n",
      "Gemini: That's wonderful! Double the love and double the fun!\n",
      "\n",
      "Do you want to tell me anything about them? Like their names, breeds, or what they're like?\n",
      "User: How many pets do I have?\n",
      "Gemini: Based on what you told me, you have **two** pets! (Two dogs, specifically.)\n",
      "\n",
      "Chat History:\n",
      "role - user: I have two dogs in my house.\n",
      "role - model: That's wonderful! Double the love and double the fun!\n",
      "\n",
      "Do you want to tell me anything about them? Like their names, breeds, or what they're like?\n",
      "role - user: How many pets do I have?\n",
      "role - model: Based on what you told me, you have **two** pets! (Two dogs, specifically.)\n"
     ]
    }
   ],
   "source": [
    "# Multi-turn conversations\n",
    "conversation = [\n",
    "    \"I have two dogs in my house.\",\n",
    "    \"How many pets do I have?\",\n",
    "]\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "for message in conversation:\n",
    "    response = chat.send_message(message)\n",
    "    print(\"User:\", message)\n",
    "    print(\"Gemini:\", response.text)\n",
    "\n",
    "\n",
    "print(\"\\nChat History:\")\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6e5523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chunk 0 ===\n",
      "Artificial Intelligence (AI) isn't a single, magic trick, but rather a collection of advanced techniques that enable machines to simulate human-like intelligence. At its core, AI works by\n",
      "=== Chunk 1 ===\n",
      " **learning from data** and then using that learned knowledge to make **predictions, classifications, or decisions** without being explicitly programmed for every single scenario.\n",
      "\n",
      "Here's a breakdown of how AI generally works:\n",
      "\n",
      "### 1. The Core Idea: Learning from Data\n",
      "\n",
      "Imagine teaching a child. You show them many\n",
      "=== Chunk 2 ===\n",
      " examples, correct them when they're wrong, and eventually, they learn to identify things on their own. AI operates similarly:\n",
      "\n",
      "*   **Data is the Fuel:** AI systems are fed vast amounts of data (images, text, numbers, sounds, etc.). The quality and quantity of this data are crucial for the\n",
      "=== Chunk 3 ===\n",
      " AI's performance.\n",
      "*   **Algorithms are the Recipes:** These are the sets of instructions or mathematical models that the AI uses to process and learn from the data.\n",
      "*   **Models are the Learned Knowledge:** After processing the data with algorithms, the AI creates a \"model.\" This model is essentially the learned\n",
      "=== Chunk 4 ===\n",
      " representation of patterns, relationships, and insights derived from the training data.\n",
      "\n",
      "### 2. How AI Learns (The Training Phase)\n",
      "\n",
      "The learning process is where AI distinguishes itself from traditional programming. There are several primary paradigms:\n",
      "\n",
      "#### a. Supervised Learning (Most Common)\n",
      "\n",
      "*   **How it works\n",
      "=== Chunk 5 ===\n",
      ":** The AI is given labeled data – meaning each piece of input data is paired with the correct output or \"answer.\" It's like having a teacher who provides both the question and the solution.\n",
      "*   **Process:**\n",
      "    1.  **Input:** The AI receives an input (e.g.,\n",
      "=== Chunk 6 ===\n",
      " an image of an animal).\n",
      "    2.  **Predict:** It tries to predict the label (e.g., \"cat\" or \"dog\").\n",
      "    3.  **Compare:** It compares its prediction to the correct label provided in the training data.\n",
      "    4.  **Adjust:** If its\n",
      "=== Chunk 7 ===\n",
      " prediction is wrong, it adjusts its internal parameters (weights and biases) to reduce the error for future predictions.\n",
      "    5.  **Repeat:** This process is repeated millions of times with different data examples until the AI model becomes highly accurate at making predictions.\n",
      "*   **Examples:** Image classification (identifying objects), spam detection (\n",
      "=== Chunk 8 ===\n",
      "classifying emails as spam or not), language translation, predicting house prices.\n",
      "\n",
      "#### b. Unsupervised Learning\n",
      "\n",
      "*   **How it works:** The AI is given unlabeled data and is tasked with finding hidden patterns, structures, or relationships within that data without any prior guidance. It's like an explorer discovering new territories\n",
      "=== Chunk 9 ===\n",
      " without a map.\n",
      "*   **Process:** The algorithms identify similarities and differences in the data to group them or reduce their complexity.\n",
      "*   **Examples:** Customer segmentation (grouping similar customers for marketing), anomaly detection (finding unusual patterns like fraud), data compression.\n",
      "\n",
      "#### c. Reinforcement Learning\n",
      "\n",
      "*   \n",
      "=== Chunk 10 ===\n",
      "**How it works:** The AI learns by trial and error, interacting with an environment. It receives rewards for desirable actions and penalties for undesirable ones. It's like training a pet with treats.\n",
      "*   **Process:**\n",
      "    1.  **Agent:** The AI acts as an \"agent\" within an \"environment.\"\n",
      "    \n",
      "=== Chunk 11 ===\n",
      "2.  **Action:** It takes an action.\n",
      "    3.  **Reward/Penalty:** The environment provides feedback in the form of a reward or penalty.\n",
      "    4.  **Optimize:** The AI learns to choose actions that maximize its cumulative reward over time.\n",
      "*   **Examples:** Training robots\n",
      "=== Chunk 12 ===\n",
      " to perform tasks, playing complex games (like Chess or Go), self-driving cars (learning to navigate).\n",
      "\n",
      "### 3. The Brain of Many AI Systems: Neural Networks & Deep Learning\n",
      "\n",
      "Many of the impressive AI breakthroughs you hear about (like image recognition, natural language processing, and advanced chatbots) are powered by **\n",
      "=== Chunk 13 ===\n",
      "Deep Learning**, which uses **Artificial Neural Networks (ANNs)**.\n",
      "\n",
      "*   **Inspired by Biology:** ANNs are loosely inspired by the structure and function of the human brain.\n",
      "*   **Layers of \"Neurons\":** They consist of multiple layers of interconnected \"nodes\" or \"neurons.\"\n",
      "    *   **Input\n",
      "=== Chunk 14 ===\n",
      " Layer:** Receives the raw data.\n",
      "    *   **Hidden Layers:** One or more layers that process the input data through complex mathematical transformations. \"Deep\" learning means having many hidden layers.\n",
      "    *   **Output Layer:** Produces the final prediction or decision.\n",
      "*   **Weights and Biases:** Each connection\n",
      "=== Chunk 15 ===\n",
      " between neurons has a \"weight,\" which determines the strength of the signal passing through it. \"Biases\" are additional values that help the network make better predictions. During training, the AI adjusts these weights and biases.\n",
      "*   **Activation Functions:** These functions introduce non-linearity into the network, allowing it to learn complex patterns that linear\n",
      "=== Chunk 16 ===\n",
      " models cannot.\n",
      "*   **Backpropagation:** This is the key algorithm used in supervised learning with neural networks. After making a prediction and comparing it to the correct answer, the error is \"propagated backward\" through the network, telling each neuron how much it contributed to the error and how to adjust its weights.\n",
      "=== Chunk 17 ===\n",
      "\n",
      "\n",
      "### 4. How AI *Does* Things (Inference/Application)\n",
      "\n",
      "Once an AI model has been trained and has learned from the data, it can be put into action:\n",
      "\n",
      "*   **New Input:** When you feed new, unseen data to the trained model (e.g., a new photo\n",
      "=== Chunk 18 ===\n",
      ", a voice command), the model uses the patterns and relationships it learned during training.\n",
      "*   **Prediction/Decision:** It processes this new input through its learned structure (its weights and biases in a neural network) to generate a prediction, classification, or decision.\n",
      "*   **Example:** A trained image recognition\n",
      "=== Chunk 19 ===\n",
      " AI sees a new picture of a cat and, based on its learned patterns from millions of other cat images, correctly identifies it as a \"cat.\"\n",
      "\n",
      "### Key Capabilities of AI:\n",
      "\n",
      "*   **Perception:** Understanding images (computer vision) and sounds (speech recognition).\n",
      "*   **Natural Language Processing (NLP):**\n",
      "=== Chunk 20 ===\n",
      " Understanding, interpreting, and generating human language (e.g., chatbots, translation).\n",
      "*   **Reasoning and Problem Solving:** Playing games, solving complex logistical problems.\n",
      "*   **Knowledge Representation:** Storing and retrieving information intelligently.\n",
      "*   **Planning and Decision Making:** Optimizing routes, scheduling, making recommendations.\n",
      "=== Chunk 21 ===\n",
      "\n",
      "\n",
      "### In Summary:\n",
      "\n",
      "AI works by **ingesting data**, using **algorithms** to find patterns, building **models** that embody this learned knowledge, and then applying these models to **make intelligent predictions or decisions** on new data. The most powerful AI systems today, especially those using deep learning, achieve this\n",
      "=== Chunk 22 ===\n",
      " by training complex neural networks on massive datasets, constantly refining their internal parameters to minimize errors and maximize accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Streaming responses\n",
    "\n",
    "response = client.models.generate_content_stream(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\"Explain how AI works\"]\n",
    ")\n",
    "\n",
    "i=0\n",
    "for chunk in response:\n",
    "    print(\"=== Chunk\",i,\"===\")\n",
    "    i+=1\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6b37d",
   "metadata": {},
   "source": [
    "# Thought Signatures\n",
    "\n",
    "Thought signatures are encrypted representations of the model's internal thought process and are used to preserve reasoning context across multi-step interactions. When using thinking models (such as the Gemini 3 and 2.5 series), the API may return a thoughtSignature field within the content parts of the response (e.g., text or functionCall parts).\n",
    "\n",
    "As a general rule, if you receive a thought signature in a model response, you should pass it back exactly as received when sending the conversation history in the next turn. When using Gemini 3 models, you must pass back thought signatures during function calling, otherwise you will get a validation error (4xx status code). This includes when using the minimal thinking level setting for Gemini 3 Flash.\n",
    "\n",
    "Note: If you use the official Google Gen AI SDKs and use the chat feature (or append the full model response object directly to history), thought signatures are handled automatically. You do not need to manually extract or manage them, or change your code.\n",
    "\n",
    "Gemini 3 returns thought signatures for all model responses (responses from the API) with a function call. Thought signatures show up in the following cases:\n",
    "\n",
    "When there are parallel function calls, the first function call part returned by the model response will have a thought signature.\n",
    "When there are sequential function calls (multi-step), each function call will have a signature and you must pass all signatures back.\n",
    "Model responses without a function call will return a thought signature inside the last part returned by the model.\n",
    "\n",
    "\n",
    "(https://ai.google.dev/gemini-api/docs/thought-signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d769f984",
   "metadata": {},
   "source": [
    "# Structured Data Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0ad9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_name='Chocolate Chip Cookies' prep_time_minutes=None ingredients=[Ingredient(name='all-purpose flour', quantity='2 and 1/4 cups'), Ingredient(name='baking soda', quantity='1 teaspoon'), Ingredient(name='salt', quantity='1 teaspoon'), Ingredient(name='unsalted butter (softened)', quantity='1 cup'), Ingredient(name='granulated sugar', quantity='3/4 cup'), Ingredient(name='packed brown sugar', quantity='3/4 cup'), Ingredient(name='vanilla extract', quantity='1 teaspoon'), Ingredient(name='large eggs', quantity='2'), Ingredient(name='semisweet chocolate chips', quantity='2 cups')] instructions=['Preheat the oven to 375°F (190°C).', 'In a small bowl, whisk together the flour, baking soda, and salt.', 'In a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.', 'Beat in the vanilla and eggs, one at a time.', 'Gradually beat in the dry ingredients until just combined.', 'Stir in the chocolate chips.', 'Drop by rounded tablespoons onto ungreased baking sheets and bake for 9 to 11 minutes.']\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Ingredient(BaseModel):\n",
    "    name: str = Field(description=\"Name of the ingredient.\")\n",
    "    quantity: str = Field(description=\"Quantity of the ingredient, including units.\")\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    recipe_name: str = Field(description=\"The name of the recipe.\")\n",
    "    prep_time_minutes: Optional[int] = Field(description=\"Optional time in minutes to prepare the recipe.\")\n",
    "    ingredients: List[Ingredient]\n",
    "    instructions: List[str]\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Please extract the recipe from the following text.\n",
    "The user wants to make delicious chocolate chip cookies.\n",
    "They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,\n",
    "1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,\n",
    "3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.\n",
    "For the best part, they'll need 2 cups of semisweet chocolate chips.\n",
    "First, preheat the oven to 375°F (190°C). Then, in a small bowl, whisk together the flour,\n",
    "baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar\n",
    "until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry\n",
    "ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons\n",
    "onto ungreased baking sheets and bake for 9 to 11 minutes.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": Recipe.model_json_schema(),\n",
    "    },\n",
    ")\n",
    "\n",
    "recipe = Recipe.model_validate_json(response.text)\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec34f1",
   "metadata": {},
   "source": [
    "# Function Calling\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/function-calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b076f",
   "metadata": {},
   "source": [
    "### From Function Declaration\n",
    "```\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cf9b0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function to call: create_bar_chart\n",
      "Arguments: {'labels': ['Q1', 'Q2', 'Q3'], 'values': [50000, 75000, 60000], 'title': 'Quarterly Sales'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Define the function declaration for the model\n",
    "create_chart_function = {\n",
    "    \"name\": \"create_bar_chart\",\n",
    "    \"description\": \"Creates a bar chart given a title, labels, and corresponding values.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The title for the chart.\",\n",
    "            },\n",
    "            \"labels\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"description\": \"List of labels for the data points (e.g., ['Q1', 'Q2', 'Q3']).\",\n",
    "            },\n",
    "            \"values\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"number\"},\n",
    "                \"description\": \"List of numerical values corresponding to the labels (e.g., [50000, 75000, 60000]).\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"title\", \"labels\", \"values\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Create a bar chart titled 'Quarterly Sales' with data: Q1: 50000, Q2: 75000, Q3: 60000.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Check for a function call\n",
    "if response.candidates[0].content.parts[0].function_call:\n",
    "    function_call = response.candidates[0].content.parts[0].function_call\n",
    "    print(f\"Function to call: {function_call.name}\")\n",
    "    print(f\"Arguments: {function_call.args}\")\n",
    "    #  In a real app, you would call your function here using a charting library:\n",
    "    #  result = create_bar_chart(**function_call.args)\n",
    "else:\n",
    "    print(\"No function call found in the response.\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e558323d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-0.5694650173187256,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            ),\n",
       "            thought_signature=b'\\n\\xff\\x02\\x01\\x8f=k_\\x1e\\xc0\\x90\\x8ePdKg\\x1b7\\xd3w\\x1e\"\\xdb\\x8aI\\xf9B/$\\xd7\\xc6V\\x7f`\\xbb\\x9d\\xcf.\\xef~\\x88o5\\x08\\x93\\'C\\xda\\x94\\xb6\\x13\\xa3\\x91^\\xbb\\xef\\xfc\\x86\\xff\\xa1\\x9e4\\xd3\\xf9\\xcc\\x85k\\x1a\\xe2\\xe4\\x8cd\\x08\\xaaB\\x0b\\tn\\x02\\x1cK\\x04\\xeb\\xf3\\x18k\\xb2\\x13|l\\xe7\\xfc\\x1c\\x07*Ig...'\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  create_time=datetime.datetime(2026, 1, 22, 3, 42, 18, 34381, tzinfo=TzInfo(0)),\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='mpxxac2MAq-Hz7sP3aj5uAU',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=10>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=20,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=20\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=138,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=138\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=112,\n",
       "    total_token_count=270,\n",
       "    traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66892230",
   "metadata": {},
   "source": [
    "## Function Execution ( The mannual way )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b081aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Step 1: Model Suggests Function Call ==================\n",
      "id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values' partial_args=None will_continue=None\n",
      "================== Step 2: Function Execution Result ==================\n",
      "Function execution result: {'brightness': 25, 'colorTemperature': 'warm'}\n",
      "================== Step 4: Wrap Function Response ==================\n",
      "media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 25,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 4a: Updated Contents ==================\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='Turn the lights down to a romantic level' thought=None thought_signature=None video_metadata=None\n",
      "Role: model, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=FunctionCall(\n",
      "  args={\n",
      "    'brightness': 25,\n",
      "    'color_temp': 'warm'\n",
      "  },\n",
      "  name='set_light_values'\n",
      ") function_response=None inline_data=None text=None thought=None thought_signature=b'\\n\\x8c\\x05\\x01\\x8f=k_\\t\\x8e8\\xd8M\\x10\\xc4\\x9eh\\x8f+#\\xb6r\\x88p\\xe2\\x18\\x85\\xeb!\\x0b\\x99=\\xb8\\x99\\x0e\\rPi\\xdc\\xac,\\xbe!\\x1c\\xaa&7E&\\xd7/7*\\xfaC\\xc6\\xf3C\\x02\\xfc\\xcb\\xfdU\\x15\\x91\\xfc\\x9fS+\\x07d\\xd7\\xe6\\xff\\xdf]&f\\xbf\\xc1d\\x0b\\x82\\xc4?|\\x19\\x03;\\xa3\\\\h\\x7f\\x1d\\x1b\\xc9+p?5mv\\xf8v7\\x02\\xb5\\x9e\\xbf\\x97\\x90B\\x9eClw\\xb8\\xe5\\xc9?\\xa7\\xf0\\x9a.\\xc3\\x16\\r\\xa9\\xa15t\\xb8[k\\xf6z\\xe7\\xa0\\x93\\x12\\x15\\xdc($\\xafz;G\\xea>\\xa1j\\xe4\\xa0\\xa9\\xec\\x83\\xb7\\xc5)\\xe9Z\\xf7\\xe9j\\xcd\\x96\\xc3\\x98\\xd7\\xb1\\x12{\\x01\\xf7\\x8a\\x88Z\\x99\\xbe7\\xcb\\xddT\\x92\\xfd\\xd9\\xde\\xd4\\x8d\\x92\\x84|\\xfd\\x94\\xf9\\x04\\xd8\\xd7\\x0bu\"HU\\xf5i\\xed_H\\x82\\x0br\\x16\\xa6\\xb8\\x90\\x81\\x0e\\xab\\xe7]\\xd8x\\xe2r\\xae\\x0c\\x15K\\xc3\\xdc\\xc8R\\xb2\\xa3s\\xa8-\\xc8<\\xdf\\xac\\x08\\x7f\\xab#F\\x9c\\x13X\\x1d1\\xda\\x85\\x13\\x0f\\xc4\\xd1\\xf5Eq\\xb4Y\\xc6\\xe6.j\\xbb3\\xa6\\x8e\\x91\\xa5Y\\xd7\\'\\xc0\\x82?\\x97\\xebA\\xd4i\\nrg\\xe2\\xe1\\xd7\\x08AF\\x0e\\x08k\\xe8,\\x9e\\xa9~\\xb7\\x89\\xbf\\x99{\\xef\\xc7@z\\xc3[w`\\x01\\xa3\\xf8\\xc0\\x7f\\xa1\\xc8c#j\\x96\\xe5\\x07\\x87N\\xa7\\xceO\\xba\\xfaa\\xbe\\xade\\x04\\xe4p\\x10\\xd4t\\xda\\xa1\\x1b\\xf43\\xa9\\x84\\xb0bJ\\x92\"\\x18\\xa4\\x04)fX\\n<\\xd1+\\x0b\\xd5\\xf9\\x18c\"]e\\xfa<_{\\xcc\\x1d\\x8dJM\\xf1e}q\\xc6\\xf8m{f9\\xb4\\xf0\\xae\\x0e\\xf6]^\\x83O\\xea\\x05\\x92Y\\x1d\\xf0<\\x95\\xbcZV\\x07\\xe7\\xef\\xca\\xf1\\xb9\\xb4\\x04\\x0c:c\\x9c\\x8d\\x85\\x87\\xf3\\xba\\xa7\\xc7\\x91\\x9f\\x9a\\x95r\\xe7\\xf0L\\xb3\\x9fT7K\\xc3Fg)[\\x14\\x13-\\x83n\\xf1\\xb2O(\\x0e\\xcc\\xe5,\\x14\\xd9\\x12\\xd0\\xee\\xd7o\\xe8\\xd2\\x9d\\xe6\\n\\t~\\xb7\\x91\\xfa\\x0fN&\\xdeGR\\xa3\\x18\\xb7^\\xd3\\x17\\xd4\\x87\\xfb\\x8c\\xc4\\x12\\x81A\\xf0b\\xc1\\xde\\x1e\\t\\xc5\\xdc.\\x93j\\xd7\\x8d\\xb3\\x89\\xfb\\xe3D.I\\xf7\\xfd\\xe1\\xccFY\\xf5\\x17\\xe6\\'\\xc0Y\\xf1\\x1d\\x9c\\x96Vp\\x0e\\x86%\\xfc\\xce{\\x871e\\x0b\\x9d\\x87\\x84F\\x1a\\'P\\x08`\\xd7V\\t7x\\xdb \\xdd\\xd2\\xa9&\\xcac-\\x90\\xc5\\xacw\\x83\\x1cR\\x04S\\xec$\\x19f\\xdfS\\x95\\x0c\\xca\\x8c\\x7f\\xe84\\xb3\\xa4\\xa8\\xce@\\x14P\\xa4\\x96\\x8c=\\x0f\\xef\\x0f\\xab\\xd8\\x80\\xa1\\xa8d,>s\\x80\\xeb\\xf0\\xf2\\xc9\\xeee7\\x8a\\xd3\\xdaG\\xe6V\\x9f\\xe9\\x1dSt\\x1b\\x8f\\xf8\\xb1\\x051<t;_\\xd2\\xca9\\xc1\\xf31HZ\"H' video_metadata=None\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 25,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 5: Final Model Response ==================\n",
      "Done. The lights are now set to a romantic level.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "# Define a function that the model can call to control smart lights\n",
    "set_light_values_declaration = {\n",
    "    \"name\": \"set_light_values\",\n",
    "    \"description\": \"Sets the brightness and color temperature of a light.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n",
    "            },\n",
    "            \"color_temp\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n",
    "                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"brightness\", \"color_temp\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# This is the actual function that would be called based on the model's suggestion\n",
    "def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
    "\n",
    "    Args:\n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "tools = types.Tool(function_declarations=[set_light_values_declaration])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Define user prompt\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"================== Step 1: Model Suggests Function Call ==================\")\n",
    "print(response.candidates[0].content.parts[0].function_call)\n",
    "\n",
    "\n",
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"set_light_values\":\n",
    "    result = set_light_values(**tool_call.args)\n",
    "    print(\"================== Step 2: Function Execution Result ==================\")\n",
    "    print(f\"Function execution result: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a function response part\n",
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "print(\"================== Step 4: Wrap Function Response ==================\")\n",
    "print(function_response_part)\n",
    "\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "print(\"================== Step 4a: Updated Contents ==================\")\n",
    "for content in contents:\n",
    "    for part in content.parts:\n",
    "        print(f'Role: {content.role}, Part: {part}')\n",
    "\n",
    "client = genai.Client()\n",
    "final_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=config,\n",
    "    contents=contents,\n",
    ")\n",
    "\n",
    "print(\"================== Step 5: Final Model Response ==================\")\n",
    "print(final_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e5cbf",
   "metadata": {},
   "source": [
    "## Parallel function calling\n",
    "\n",
    "In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5cd5449",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_disco_ball = {\n",
    "    \"name\": \"power_disco_ball\",\n",
    "    \"description\": \"Powers the spinning disco ball.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"power\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether to turn the disco ball on or off.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"power\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "start_music = {\n",
    "    \"name\": \"start_music\",\n",
    "    \"description\": \"Play some music matching the specified parameters.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"energetic\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is energetic or not.\",\n",
    "            },\n",
    "            \"loud\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is loud or not.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"energetic\", \"loud\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "dim_lights = {\n",
    "    \"name\": \"dim_lights\",\n",
    "    \"description\": \"Dim the lights.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"brightness\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7f87055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Forced function calling\n",
      "power_disco_ball(power=True)\n",
      "start_music(loud=True, energetic=True)\n",
      "dim_lights(brightness=0.3)\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Configure the client and tools\n",
    "client = genai.Client()\n",
    "house_tools = [\n",
    "    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n",
    "]\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=house_tools,\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
    "        disable=True\n",
    "    ),\n",
    "    # Force the model to call 'any' function, instead of chatting.\n",
    "    tool_config=types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\", config=config)\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call\n",
    "print(\"Example 1: Forced function calling\")\n",
    "for fn in response.function_calls:\n",
    "    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "    print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb78cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-1.0244757652282714,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            ),\n",
       "            thought_signature=b'\\n\\xba\\x03\\x01\\x8f=k_\\xd9\\xde\\xf6\\x0c4>J\\x88}e-\\xd0g\\n\\xd1\\xba\\t\\x8aX\\xa2\\t\\xff\\x17B\\\\WxO\\xc3=O\\x81\\x11]/\\xf6\\xd3\\x83\\xe4\\x7fZ\\xe4C\\x03Z\\xcc\\xa6\\x9dH2\\x81\\xdaM\\xcd\\xd8\\xcf\\xfe\\xb7\\xbd\\xefC3 \\x95\\t\\x81\\x92\\x93(_VUS\\xceuM\\x1b\\xfd\\x1eK\\xe5\\xfa\\x7f\\xa7\\xf3\\\\\\xf3\\xdf\\x18...'\n",
       "          ),\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            )\n",
       "          ),\n",
       "          Part(\n",
       "            function_call=FunctionCall(\n",
       "              args=<... Max depth ...>,\n",
       "              name=<... Max depth ...>\n",
       "            )\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  create_time=datetime.datetime(2026, 1, 22, 4, 19, 3, 198572, tzinfo=TzInfo(0)),\n",
       "  model_version='gemini-2.5-flash',\n",
       "  response_id='N6VxaayPDO6bseMPy4eRqAo',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=10>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=20,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=20\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=99,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=99\n",
       "      ),\n",
       "    ],\n",
       "    thoughts_token_count=121,\n",
       "    total_token_count=240,\n",
       "    traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1afa08",
   "metadata": {},
   "source": [
    "## Automatic Function Calls\n",
    "The Python SDK supports automatic function calling, which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.\n",
    "\n",
    "In the mannual way we were doing\n",
    "\n",
    "```\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "```\n",
    "\n",
    "Now with automatic function calling we can do\n",
    "\n",
    "```\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdb009ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: power_disco_ball_impl\n",
      "Function called: start_music_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, the party is starting! The disco ball is spinning, energetic and loud music is playing, and the lights have been dimmed to 30%. Let's get this party going!\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "power_disco_ball_state = None\n",
    "start_music_state = None\n",
    "dim_lights_state = None\n",
    "\n",
    "\n",
    "# Actual function implementations\n",
    "def power_disco_ball_impl(power: bool) -> dict:\n",
    "    \"\"\"Powers the spinning disco ball.\n",
    "\n",
    "    Args:\n",
    "        power: Whether to turn the disco ball on or off.\n",
    "\n",
    "    Returns:\n",
    "        A status dictionary indicating the current state.\n",
    "    \"\"\"\n",
    "    print(\"Function called: power_disco_ball_impl\")\n",
    "    global power_disco_ball_state\n",
    "    power_disco_ball_state =  {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "    return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "\n",
    "def start_music_impl(energetic: bool, loud: bool) -> dict:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        energetic: Whether the music is energetic or not.\n",
    "        loud: Whether the music is loud or not.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the music settings.\n",
    "    \"\"\"\n",
    "    print(\"Function called: start_music_impl\")\n",
    "    music_type = \"energetic\" if energetic else \"chill\"\n",
    "    volume = \"loud\" if loud else \"quiet\"\n",
    "    global start_music_state\n",
    "    start_music_state = {\"music_type\": music_type, \"volume\": volume}\n",
    "    return {\"music_type\": music_type, \"volume\": volume}\n",
    "\n",
    "def dim_lights_impl(brightness: float) -> dict:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the new brightness setting.\n",
    "    \"\"\"\n",
    "    print(\"Function called: dim_lights_impl\")\n",
    "    global dim_lights_state\n",
    "    dim_lights_state = {\"brightness\": brightness}\n",
    "    return {\"brightness\": brightness}\n",
    "\n",
    "# Configure the client\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party!\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec9f5f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'status': 'Disco ball powered on'},\n",
       " {'music_type': 'energetic', 'volume': 'loud'},\n",
       " {'brightness': 0.3})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_disco_ball_state, start_music_state, dim_lights_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb7164",
   "metadata": {},
   "source": [
    "#### Trying to manipulate the order of Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6fa288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Party started! The music is loud and energetic, the disco ball is spinning, and the lights are at 50% brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8e20df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, I've set up the party: energetic and loud music, disco ball on, and lights at 50%.\n",
      "\n",
      "Then, I've wound things down: chill and quiet music, disco ball off, and lights at full brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness. Then again start the music quietly and chill, turn off the disco ball, and set the lights to full brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e919e35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'status': 'Disco ball powered off'},\n",
       " {'music_type': 'chill', 'volume': 'quiet'},\n",
       " {'brightness': 1})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_disco_ball_state, start_music_state, dim_lights_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74d6f11",
   "metadata": {},
   "source": [
    "## Compositional function calling\n",
    "\n",
    "Passing the output of one function as the input to another function. This is useful when the functions are dependent on each other and the output of one function is needed as input for another function. For example, you can first retrieve user information and then use that information to generate a personalized recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "287385a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather_forecast(location=London)\n",
      "Tool Response: {'temperature': 25, 'unit': 'celsius'}\n",
      "Tool Call: set_thermostat_temperature(temperature=20)\n",
      "Tool Response: {'status': 'success'}\n",
      "The thermostat has been set to 20°C.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Example Functions\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n",
    "    print(f\"Tool Call: get_weather_forecast(location={location})\")\n",
    "    # TODO: Make API call\n",
    "    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n",
    "    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n",
    "    # TODO: Interact with a thermostat API\n",
    "    print(\"Tool Response: {'status': 'success'}\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "# Configure the client and model\n",
    "client = genai.Client()\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"If it's warmer than 20°C in London, set the thermostat to 20°C, otherwise set it to 18°C.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the final, user-facing response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ced19",
   "metadata": {},
   "source": [
    "## Final Notes for Function Calling\n",
    "\n",
    "### Modes\n",
    "The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the.function_calling_config.\n",
    "\n",
    "-AUTO (Default): The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n",
    "- ANY: The model is constrained to always predict a function call and guarantees function schema adherence. If allowed_function_names is not specified, the model can choose from any of the provided function declarations. If allowed_function_names is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).\n",
    "- NONE: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n",
    "- VALIDATED (Preview): The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions.\n",
    "\n",
    "Example usage:\n",
    "\n",
    "```\n",
    "# Configure function calling mode\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config=types.FunctionCallingConfig(\n",
    "        mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the generation config\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[tools],  # not defined here.\n",
    "    tool_config=tool_config,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "### Combine native tools with function calling\n",
    "\n",
    "Multi-tool use is a-Live API only feature at the moment. The run() function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n",
    "\n",
    "```\n",
    "# Multiple tasks example - combining lights, code execution, and search\n",
    "prompt = \"\"\"\n",
    "  Hey, I need you to do three things for me.\n",
    "\n",
    "    1.  Turn on the lights.\n",
    "    2.  Then compute the largest prime palindrome under 100000.\n",
    "    3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n",
    "\n",
    "  Thanks!\n",
    "  \"\"\"\n",
    "\n",
    "tools = [\n",
    "    {'google_search': {}},\n",
    "    {'code_execution': {}},\n",
    "    {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n",
    "]\n",
    "\n",
    "# Execute the prompt with specified tools in audio modality\n",
    "await run(prompt, tools=tools, modality=\"AUDIO\")\n",
    "```\n",
    "\n",
    "### Live API: https://ai.google.dev/gemini-api/docs/live?example=mic-stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df36896",
   "metadata": {},
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API provides a code execution tool that enables the model to generate and run Python code. The model can then learn iteratively from the code execution results until it arrives at a final output. You can use code execution to build applications that benefit from code-based reasoning. For example, you can use code execution to solve equations or process text. You can also use the libraries included in the code execution environment to perform more specialized tasks.\n",
    "\n",
    "Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution\n",
    "\n",
    "- Supported libraries\n",
    "The code execution environment includes the following libraries:\n",
    "\n",
    "attrs\n",
    "chess\n",
    "contourpy\n",
    "fpdf\n",
    "geopandas\n",
    "imageio\n",
    "jinja2\n",
    "joblib\n",
    "jsonschema\n",
    "jsonschema-specifications\n",
    "lxml\n",
    "matplotlib\n",
    "mpmath\n",
    "numpy\n",
    "opencv-python\n",
    "openpyxl\n",
    "packaging\n",
    "pandas\n",
    "pillow\n",
    "protobuf\n",
    "pylatex\n",
    "pyparsing\n",
    "PyPDF2\n",
    "python-dateutil\n",
    "python-docx\n",
    "python-pptx\n",
    "reportlab\n",
    "scikit-learn\n",
    "scipy\n",
    "seaborn\n",
    "six\n",
    "striprtf\n",
    "sympy\n",
    "tabulate\n",
    "tensorflow\n",
    "toolz\n",
    "xlrd\n",
    "\n",
    "###### You can't install your own libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7ee5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready! Go ahead and share it with me. Whether it's algebra, calculus, geometry, statistics, or anything else, I'll do my best to help you solve it or explain the concepts involved.\n",
      "def is_prime(n):\n",
      "    if n < 2:\n",
      "        return False\n",
      "    for i in range(2, int(n**0.5) + 1):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "primes = []\n",
      "num = 2\n",
      "while len(primes) < 50:\n",
      "    if is_prime(num):\n",
      "        primes.append(num)\n",
      "    num += 1\n",
      "\n",
      "print(f\"The first 50 primes are: {primes}\")\n",
      "print(f\"The number of primes found: {len(primes)}\")\n",
      "print(f\"The sum of the first 50 primes is: {sum(primes)}\")\n",
      "The first 50 primes are: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\n",
      "The number of primes found: 50\n",
      "The sum of the first 50 primes is: 5117\n",
      "\n",
      "The sum of the first 50 prime numbers is **5,117**.\n",
      "\n",
      "To find this, I used a script to identify the primes starting from 2 and stopped once the 50th prime (which is 229) was reached. \n",
      "\n",
      "**The first 50 prime numbers are:**\n",
      "2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229.\n",
      "\n",
      "**Calculation:**\n",
      "$2 + 3 + 5 + \\dots + 229 = 5,117$\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "client = genai.Client()\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=\"gemini-3-flash-preview\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat.send_message(\"I have a math question for you.\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"What is the sum of the first 50 prime numbers? \"\n",
    "    \"Generate and run code for the calculation, and make sure you get all 50.\"\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "        print(part.text)\n",
    "    if part.executable_code is not None:\n",
    "        print(part.executable_code.code)\n",
    "    if part.code_execution_result is not None:\n",
    "        print(part.code_execution_result.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htll-ir-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
