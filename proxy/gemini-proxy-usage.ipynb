{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce24e7aa",
   "metadata": {},
   "source": [
    "# Using GenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b350ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "PROJECT_ID = \"cloud-ai-police\"\n",
    "LOCATION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f3c13",
   "metadata": {},
   "source": [
    "### Baics and CLient Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1448e818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! Your connection is coming through loud and clear. It's great to hear from you.\n",
      "\n",
      "How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    http_options={\n",
    "        # ðŸ‘‡ this replaces https://{location}-aiplatform.googleapis.com\n",
    "        \"base_url\": \"http://localhost:8080/gemini\",\n",
    "        # optional but useful\n",
    "        \"timeout\": 120_000,\n",
    "    },\n",
    ")\n",
    "\n",
    "resp = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=\"Hello via local proxy!\",\n",
    ")\n",
    "\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd7e66",
   "metadata": {},
   "source": [
    "### Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965ff955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",  # Thinking supported in this model\n",
    "    contents=\"Provide a list of 3 famous physicists and their key contributions\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=1024)\n",
    "        # Turn off thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        # Turn on dynamic thinking:\n",
    "        # thinking_config=types.ThinkingConfig(thinking_budget=-1)\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5991d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three famous physicists and their key contributions:\n",
      "\n",
      "1.  **Sir Isaac Newton**\n",
      "    *   **Key Contributions:**\n",
      "        *   **Laws of Motion:** Formulated the three fundamental laws governing motion (inertia, F=ma, action-reaction) which laid the foundation for classical mechanics.\n",
      "        *   **Universal Gravitation:** Developed the law of universal gravitation, explaining the force that attracts any two objects with mass, from an apple falling to the Earth to the orbits of planets.\n",
      "        *   **Calculus:** Independently developed integral and differential calculus, a crucial mathematical tool for understanding physical change.\n",
      "        *   **Optics:** Made significant contributions to the study of light, demonstrating that white light is composed of a spectrum of colors.\n",
      "\n",
      "2.  **Albert Einstein**\n",
      "    *   **Key Contributions:**\n",
      "        *   **Theories of Relativity:** Developed the Special Theory of Relativity (1905), which redefined space and time, introducing concepts like time dilation and length contraction, and the famous mass-energy equivalence formula, E=mcÂ². He later developed the General Theory of Relativity (1915), which redefined gravity as a curvature of spacetime caused by mass and energy.\n",
      "        *   **Photoelectric Effect:** Explained the photoelectric effect (1905), where light causes electrons to be ejected from a material, by proposing that light consists of discrete packets of energy called photons. This was a crucial step in the development of quantum mechanics and earned him the Nobel Prize in Physics.\n",
      "        *   **Brownian Motion:** Provided a theoretical explanation for Brownian motion (1905), the random movement of particles in a fluid, which helped to confirm the existence of atoms and molecules.\n",
      "\n",
      "3.  **Marie SkÅ‚odowska Curie**\n",
      "    *   **Key Contributions:**\n",
      "        *   **Radioactivity:** Pioneered research into radioactivity, a term she coined. Her work provided the foundational understanding of radioactive decay.\n",
      "        *   **Discovery of New Elements:** Discovered two new radioactive elements, polonium and radium, with her husband Pierre Curie.\n",
      "        *   **Isolation of Radium:** Successfully isolated pure radium metal, proving its existence and allowing for further study of its properties.\n",
      "        *   **Medical Applications:** Advocated for the use of radium for treating cancer and, during World War I, developed mobile X-ray units (\"Petites Curies\") for diagnosing injuries on the battlefield. She was the first woman to win a Nobel Prize, the first person and only woman to win Nobel Prizes in two different scientific fields (Physics in 1903 and Chemistry in 1911), and the only person to win Nobel Prizes in multiple sciences.\n",
      "Thoughts tokens: 817\n",
      "Output tokens: 559\n"
     ]
    }
   ],
   "source": [
    "print(response.text)\n",
    "print(\"Thoughts tokens:\",response.usage_metadata.thoughts_token_count)\n",
    "print(\"Output tokens:\",response.usage_metadata.candidates_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ee62b",
   "metadata": {},
   "source": [
    "### Adding system instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061cce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mrow! *blinks slowly, then gives a tiny head-nuzzle against the invisible air* Hello to you too!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a cat. Your name is Neko.\"),\n",
    "    contents=\"Hello there\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d526b5",
   "metadata": {},
   "source": [
    "### Multi-turn conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd045f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I have two dogs in my house.\n",
      "Gemini: Oh, how wonderful! Having two dogs must bring so much joy (and probably some lively moments!) to your home.\n",
      "\n",
      "What kind of dogs are they? Do they have names? I'd love to hear more about them if you'd like to share!\n",
      "User: How many pets do I have?\n",
      "Gemini: Based on what you told me, you have **two** pets (your two dogs!).\n",
      "\n",
      "Chat History:\n",
      "role - user: I have two dogs in my house.\n",
      "role - model: Oh, how wonderful! Having two dogs must bring so much joy (and probably some lively moments!) to your home.\n",
      "\n",
      "What kind of dogs are they? Do they have names? I'd love to hear more about them if you'd like to share!\n",
      "role - user: How many pets do I have?\n",
      "role - model: Based on what you told me, you have **two** pets (your two dogs!).\n"
     ]
    }
   ],
   "source": [
    "conversation = [\n",
    "    \"I have two dogs in my house.\",\n",
    "    \"How many pets do I have?\",\n",
    "]\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\")\n",
    "\n",
    "for message in conversation:\n",
    "    response = chat.send_message(message)\n",
    "    print(\"User:\", message)\n",
    "    print(\"Gemini:\", response.text)\n",
    "\n",
    "\n",
    "print(\"\\nChat History:\")\n",
    "\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dc6b60",
   "metadata": {},
   "source": [
    "### Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f4c3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recipe_name='delicious chocolate chip cookies' prep_time_minutes=None ingredients=[Ingredient(name='all-purpose flour', quantity='2 and 1/4 cups'), Ingredient(name='baking soda', quantity='1 teaspoon'), Ingredient(name='salt', quantity='1 teaspoon'), Ingredient(name='unsalted butter', quantity='1 cup'), Ingredient(name='granulated sugar', quantity='3/4 cup'), Ingredient(name='packed brown sugar', quantity='3/4 cup'), Ingredient(name='vanilla extract', quantity='1 teaspoon'), Ingredient(name='large eggs', quantity='2'), Ingredient(name='semisweet chocolate chips', quantity='2 cups')] instructions=['Preheat the oven to 375Â°F (190Â°C).', 'In a small bowl, whisk together the flour, baking soda, and salt.', 'In a large bowl, cream together the butter, granulated sugar, and brown sugar until light and fluffy.', 'Beat in the vanilla and eggs, one at a time.', 'Gradually beat in the dry ingredients until just combined.', 'Stir in the chocolate chips.', 'Drop by rounded tablespoons onto ungreased baking sheets and bake for 9 to 11 minutes.']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class Ingredient(BaseModel):\n",
    "    name: str = Field(description=\"Name of the ingredient.\")\n",
    "    quantity: str = Field(description=\"Quantity of the ingredient, including units.\")\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    recipe_name: str = Field(description=\"The name of the recipe.\")\n",
    "    prep_time_minutes: Optional[int] = Field(description=\"Optional time in minutes to prepare the recipe.\")\n",
    "    ingredients: List[Ingredient]\n",
    "    instructions: List[str]\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Please extract the recipe from the following text.\n",
    "The user wants to make delicious chocolate chip cookies.\n",
    "They need 2 and 1/4 cups of all-purpose flour, 1 teaspoon of baking soda,\n",
    "1 teaspoon of salt, 1 cup of unsalted butter (softened), 3/4 cup of granulated sugar,\n",
    "3/4 cup of packed brown sugar, 1 teaspoon of vanilla extract, and 2 large eggs.\n",
    "For the best part, they'll need 2 cups of semisweet chocolate chips.\n",
    "First, preheat the oven to 375Â°F (190Â°C). Then, in a small bowl, whisk together the flour,\n",
    "baking soda, and salt. In a large bowl, cream together the butter, granulated sugar, and brown sugar\n",
    "until light and fluffy. Beat in the vanilla and eggs, one at a time. Gradually beat in the dry\n",
    "ingredients until just combined. Finally, stir in the chocolate chips. Drop by rounded tablespoons\n",
    "onto ungreased baking sheets and bake for 9 to 11 minutes.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": Recipe.model_json_schema(),\n",
    "    },\n",
    ")\n",
    "\n",
    "recipe = Recipe.model_validate_json(response.text)\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065fd43",
   "metadata": {},
   "source": [
    "### function calling ( the mannual way )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aadb2897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Step 1: Model Suggests Function Call ==================\n",
      "id=None args={'brightness': 20, 'color_temp': 'warm'} name='set_light_values' partial_args=None will_continue=None\n",
      "================== Step 2: Function Execution Result ==================\n",
      "Function execution result: {'brightness': 20, 'colorTemperature': 'warm'}\n",
      "================== Step 4: Wrap Function Response ==================\n",
      "media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 20,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 4a: Updated Contents ==================\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=None inline_data=None text='Turn the lights down to a romantic level' thought=None thought_signature=None video_metadata=None\n",
      "Role: model, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=FunctionCall(\n",
      "  args={\n",
      "    'brightness': 20,\n",
      "    'color_temp': 'warm'\n",
      "  },\n",
      "  name='set_light_values'\n",
      ") function_response=None inline_data=None text=None thought=None thought_signature=b'\\n\\x88\\x06\\x01\\x8f=k_\\xc8\\xd9`\\xb21\\x1f3\\x18=\\xd1 u\\x92[\\xb8\\x9acW;w`\\xfa\\xbd\\xce\\x8d\\x86\\x13\\x8d\\xe9\\xcb\\x1f\\xf8(B}\\x05\\r\\x85\\xc5)=\\xeb\\xdc/\\x1c\\xe70>r7\\xbcO\\x13\\x87\\xc4jz\\xfa\\x1b~\\xc9\\xce\\xfds\\x16\\x00\\x1e\\xebg\\xfd\\x9d\\x92\\x84\\x9d=M(v*\\x15\\x1e\\x9f\\x15\\xce\\x8b\\x87\\xdeY\\xae\\x14\"V\\xad$\\x12\\x94JA\\xe6pj\\xcet\\x12\\xc1\\xcbi~1\\xf8\\x1ap\\xe0\\x05\\xb2\\xb0d4\\x12\\xc7\\xfe\\x95J\\x87F%l\\x18\\x82B~\\xd6\\x91}#\\xacE\\xdf\\xe44u\\xe0\\xcc\\xdd\\x1bB\\xdd4\\x99\\x13fck\\xccQ\\xd6w\\xa1\\xc3\\xec\\tL\\xb0\\x08\\xf8/\\x9e\\x15;\\x98sw\\xf9\\x84\\xa3\\xd4\\x15\\xf8\\xebX\\xb0\\x8c\\x8c+\\xbcs\\x94\\xcb:\\x88O/\\x98\\x87\\x989H3\\xde\\x82H\\xdd\\x94\\xccgL\\xeb\\xb1\\x8bh\\xfe}\\xee2\\x95u@\\xd8$ \\x06\"\\xca-\\x7f\\xac\\xc2\\xae2\\xa6\\x11\\x89Y\\\\\\xb9\\x064@\\xccTk\\xbc)\\xe0\\xd8mO\\x89\\x11A\\xabV*\\x00:\"\\x9c6\\x07\\xe7!\\xcf\\xee\\xc18d.\\xcd\\xae\\x87\\'\\xb8\\xa7\\xeb\\xb2\\xdd\\x00\\xc0\\x13\\xa0\\xf5\\x01\\x12\\xf0b;\\xe8NQ\\x8a\\x06\\x82\\x1e\\x13\\x02\\xeeM>\\x84\\x12\\xd8.\\xa3\\xd9\\xa63\\xedB\\xe0@L\\xcd\\x17h\\xece\\xa4\\x8c\\xfd[3\\xb6\\xf6\\xb4\\x05T\\x9a\\xd2W\\x17d\\xd6\\n\\xd4o\\x9f\\xa0z/Z\\xb4\\x8ec\\xbbl\\xabbN\\xc6\\xfa|;\\xf6;\\xee\\xa4\\x0c\\xae\\xd2\\xa5\\xb9\\xcf\\x7fq\\\\\\x01\\xaf\\xec\\x98\\xe0-\\xfa~\\x80\\x8f\\x1f\\x87\\xff\\xbf\\x8f\\xe2yr6\\xb2\\x0f\\xcb\\xe2\\xf4\\x06\\xe1\\x88$`\\xdcy\\xb5\\xe6s\\xb1\\x17q\\xf4\\xb9\\x17?7[\\xdc\\x84\\xc4W\\xf0\\xf0\\xae\\xc8G\\x13\\xfbn~E\\x91NSQ\\xce2U\\x15\\xf5H\\x80_\\x0c\\xc1\"\\xab\\xe1\\x08\\x80\\x8dE\\xeb\\xc0O\\xbd\\x16H\\xe0\\x06o\\x8d\\xa5\\x08%\\xc7\\x06z\\xf3/\\xaf^\\x00\\x0c\\xb0\\xbaX\\xd7Lz+z\\xdf\\xb5\\x0c*\\x08\\xf1\\xe0\\xe6\\xb902_\\x87\\x85\\xbd\\x82~\\x94q\\xb2\\x96\\x93\\xe6\\x7fp~\\x01?\\x03\\x18Q\\xfb\\x18C!\\x9c\\x0c\\xb4\\x1a\\xddel\\xa5\\x99\\xa2&XV\\x8c\\xcdc\\x02\\xa4\\xa5\\xd26\\xe5\\x1eF@\\xeaa]\\x16\\xcaL\\xab\\xca\\xf9\\xc9\\xc6\\xea\\xffg\\xbf\\xe5\\xb0M~\\x85F\\xa8\\x14kO\\x0fv\\xfd\\xb4\\x90\\xf0\\x90-\\xe0\\xed\\x8d\\x9eO\\xfb]\\xaeE\\\\\\xe8\\xd4\\x02\\x8by\\xe8\\xf2\\xc2Q\\x10H\\xde\\xf6w\\xe2\\x03\\xf4uoM\\xeb!\\x12\\xbc\\xcfR\\xdd\\xe3\\xf0\\x88\\xc6\\xbb\\xc8[\\x1e\\x06\\xc4\\xe5\\xff{(\\xe7\\xb1\\xabw\\xae\\xe7\\x12`R\\x06\\xfc[Dl\\x7fa\\x9b\\xf7\\xd6m\\xfc\\xe9\\x8b\\x15\\xac\\x08\\xe9\\x9cJ\\x92\\x0c\\x03| \\xdb\\x887E\\x81>\\t\\x9bJ\\xf8\\x900\\x04\\'\\x02\\xaaZ]\\x7fF\\xc7\\x1b\\xad\\x8a\\x1b\\x9a\\xec\\xcf\\x00\\xf15d\\x82\\xcch7\\xa4\\x1d\\xee\\xd4\\xa0\\x82\\x17\\xde\\xaf\\x7f\\xe4\\x12\\xa5\\xd4\\xf0H\\xb4Ja\\xe6&\\xc9\\x82\\x91,Y\\xac(3K\\x90\\x8c&\\x82*\\x1e\\xa8\\x07\\x03\\x94a\\x02<\\x16\\xa9\\x0c\\r\\xce\\xf3\\xadp\\xades\\x03\\xe2\\xf9D\\x9es\\xa0\\xa45\\xad?\\xd4\\x12P\\xb3Q\\xee\\xf8?\\x16>\\xba\\xdc`\\xf5\\x8b\\xd9\\xae\\xb8J' video_metadata=None\n",
      "Role: user, Part: media_resolution=None code_execution_result=None executable_code=None file_data=None function_call=None function_response=FunctionResponse(\n",
      "  name='set_light_values',\n",
      "  response={\n",
      "    'result': {\n",
      "      'brightness': 20,\n",
      "      'colorTemperature': 'warm'\n",
      "    }\n",
      "  }\n",
      ") inline_data=None text=None thought=None thought_signature=None video_metadata=None\n",
      "================== Step 5: Final Model Response ==================\n",
      "The lights are now set to a warm, romantic glow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a function that the model can call to control smart lights\n",
    "set_light_values_declaration = {\n",
    "    \"name\": \"set_light_values\",\n",
    "    \"description\": \"Sets the brightness and color temperature of a light.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n",
    "            },\n",
    "            \"color_temp\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n",
    "                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"brightness\", \"color_temp\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# This is the actual function that would be called based on the model's suggestion\n",
    "def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n",
    "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
    "\n",
    "    Args:\n",
    "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
    "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the set brightness and color temperature.\n",
    "    \"\"\"\n",
    "    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tools = types.Tool(function_declarations=[set_light_values_declaration])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Define user prompt\n",
    "contents = [\n",
    "    types.Content(\n",
    "        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Send request with function declarations\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=contents,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"================== Step 1: Model Suggests Function Call ==================\")\n",
    "print(response.candidates[0].content.parts[0].function_call)\n",
    "\n",
    "\n",
    "# Extract tool call details, it may not be in the first part.\n",
    "tool_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if tool_call.name == \"set_light_values\":\n",
    "    result = set_light_values(**tool_call.args)\n",
    "    print(\"================== Step 2: Function Execution Result ==================\")\n",
    "    print(f\"Function execution result: {result}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create a function response part\n",
    "function_response_part = types.Part.from_function_response(\n",
    "    name=tool_call.name,\n",
    "    response={\"result\": result},\n",
    ")\n",
    "print(\"================== Step 4: Wrap Function Response ==================\")\n",
    "print(function_response_part)\n",
    "\n",
    "# Append function call and result of the function execution to contents\n",
    "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
    "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
    "\n",
    "print(\"================== Step 4a: Updated Contents ==================\")\n",
    "for content in contents:\n",
    "    for part in content.parts:\n",
    "        print(f'Role: {content.role}, Part: {part}')\n",
    "\n",
    "\n",
    "final_response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=config,\n",
    "    contents=contents,\n",
    ")\n",
    "\n",
    "print(\"================== Step 5: Final Model Response ==================\")\n",
    "print(final_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8e5e2",
   "metadata": {},
   "source": [
    "### Parallel function calling ( Mannual way)\n",
    "\n",
    "In addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640b522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_disco_ball = {\n",
    "    \"name\": \"power_disco_ball\",\n",
    "    \"description\": \"Powers the spinning disco ball.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"power\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether to turn the disco ball on or off.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"power\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "start_music = {\n",
    "    \"name\": \"start_music\",\n",
    "    \"description\": \"Play some music matching the specified parameters.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"energetic\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is energetic or not.\",\n",
    "            },\n",
    "            \"loud\": {\n",
    "                \"type\": \"boolean\",\n",
    "                \"description\": \"Whether the music is loud or not.\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"energetic\", \"loud\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "dim_lights = {\n",
    "    \"name\": \"dim_lights\",\n",
    "    \"description\": \"Dim the lights.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"brightness\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"brightness\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a08d5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Forced function calling\n",
      "start_music(energetic=True, loud=True)\n",
      "power_disco_ball(power=True)\n",
      "dim_lights(brightness=0.3)\n"
     ]
    }
   ],
   "source": [
    "house_tools = [\n",
    "    types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n",
    "]\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=house_tools,\n",
    "    automatic_function_calling=types.AutomaticFunctionCallingConfig(\n",
    "        disable=True\n",
    "    ),\n",
    "    # Force the model to call 'any' function, instead of chatting.\n",
    "    tool_config=types.ToolConfig(\n",
    "        function_calling_config=types.FunctionCallingConfig(mode='ANY')\n",
    "    ),\n",
    ")\n",
    "\n",
    "chat = client.chats.create(model=\"gemini-2.5-flash\", config=config)\n",
    "response = chat.send_message(\"Turn this place into a party!\")\n",
    "\n",
    "# Print out each of the function calls requested from this single call\n",
    "print(\"Example 1: Forced function calling\")\n",
    "for fn in response.function_calls:\n",
    "    args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n",
    "    print(f\"{fn.name}({args})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9912c5",
   "metadata": {},
   "source": [
    "### Function Call (Automatic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48733b2d",
   "metadata": {},
   "source": [
    "The Python SDK supports automatic function calling, which automatically converts Python functions to declarations, handles the function call execution and response cycle for you. Following is an example for the disco use case.\n",
    "\n",
    "In the mannual way we were doing\n",
    "\n",
    "```\n",
    "tools = types.Tool(function_declarations=[create_chart_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "```\n",
    "\n",
    "Now with automatic function calling we can do\n",
    "\n",
    "```\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7939027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_disco_ball_state = None\n",
    "start_music_state = None\n",
    "dim_lights_state = None\n",
    "\n",
    "\n",
    "# Actual function implementations\n",
    "def power_disco_ball_impl(power: bool) -> dict:\n",
    "    \"\"\"Powers the spinning disco ball.\n",
    "\n",
    "    Args:\n",
    "        power: Whether to turn the disco ball on or off.\n",
    "\n",
    "    Returns:\n",
    "        A status dictionary indicating the current state.\n",
    "    \"\"\"\n",
    "    print(\"Function called: power_disco_ball_impl\")\n",
    "    global power_disco_ball_state\n",
    "    power_disco_ball_state =  {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "    return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
    "\n",
    "def start_music_impl(energetic: bool, loud: bool) -> dict:\n",
    "    \"\"\"Play some music matching the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        energetic: Whether the music is energetic or not.\n",
    "        loud: Whether the music is loud or not.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the music settings.\n",
    "    \"\"\"\n",
    "    print(\"Function called: start_music_impl\")\n",
    "    music_type = \"energetic\" if energetic else \"chill\"\n",
    "    volume = \"loud\" if loud else \"quiet\"\n",
    "    global start_music_state\n",
    "    start_music_state = {\"music_type\": music_type, \"volume\": volume}\n",
    "    return {\"music_type\": music_type, \"volume\": volume}\n",
    "\n",
    "def dim_lights_impl(brightness: float) -> dict:\n",
    "    \"\"\"Dim the lights.\n",
    "\n",
    "    Args:\n",
    "        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the new brightness setting.\n",
    "    \"\"\"\n",
    "    print(\"Function called: dim_lights_impl\")\n",
    "    global dim_lights_state\n",
    "    dim_lights_state = {\"brightness\": brightness}\n",
    "    return {\"brightness\": brightness}\n",
    "\n",
    "# Configure the client\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92024b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: power_disco_ball_impl\n",
      "Function called: start_music_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, the disco ball is spinning, the music is pumping, and the lights are set for a party!\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party!\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77868af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "The party is ready! The music is loud and energetic, the disco ball is on, and the lights are at 50% brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f242e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "Function called: start_music_impl\n",
      "Function called: power_disco_ball_impl\n",
      "Function called: dim_lights_impl\n",
      "\n",
      "Automatic function calling\n",
      "Alright, I've set up the party: loud, energetic music is playing, the disco ball is on, and the lights are at 50% brightness.\n",
      "\n",
      "Then, I've returned things to normal: quiet, chill music is playing, the disco ball is off, and the lights are at full brightness.\n"
     ]
    }
   ],
   "source": [
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Do everything you need to this place into party! First start the music loudly and energetically, then turn on the disco ball, and finally dim the lights to 50% brightness. Then again start the music quietly and chill, turn off the disco ball, and set the lights to full brightness.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(\"\\nAutomatic function calling\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c865ee",
   "metadata": {},
   "source": [
    "### Compositional function calling ( Automatic )\n",
    "\n",
    "Passing the output of one function as the input to another function. This is useful when the functions are dependent on each other and the output of one function is needed as input for another function. For example, you can first retrieve user information and then use that information to generate a personalized recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9085a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call: get_weather_forecast(location=London)\n",
      "Tool Response: {'temperature': 25, 'unit': 'celsius'}\n",
      "Tool Call: set_thermostat_temperature(temperature=20)\n",
      "Tool Response: {'status': 'success'}\n",
      "The thermostat has been set to 20Â°C.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# Example Functions\n",
    "def get_weather_forecast(location: str) -> dict:\n",
    "    \"\"\"Gets the current weather temperature for a given location.\"\"\"\n",
    "    print(f\"Tool Call: get_weather_forecast(location={location})\")\n",
    "    # TODO: Make API call\n",
    "    print(\"Tool Response: {'temperature': 25, 'unit': 'celsius'}\")\n",
    "    return {\"temperature\": 25, \"unit\": \"celsius\"}  # Dummy response\n",
    "\n",
    "def set_thermostat_temperature(temperature: int) -> dict:\n",
    "    \"\"\"Sets the thermostat to a desired temperature.\"\"\"\n",
    "    print(f\"Tool Call: set_thermostat_temperature(temperature={temperature})\")\n",
    "    # TODO: Interact with a thermostat API\n",
    "    print(\"Tool Response: {'status': 'success'}\")\n",
    "    return {\"status\": \"success\"}\n",
    "\n",
    "# Configure the client and model\n",
    "config = types.GenerateContentConfig(\n",
    "    tools=[get_weather_forecast, set_thermostat_temperature]\n",
    ")\n",
    "\n",
    "# Make the request\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"If it's warmer than 20Â°C in London, set the thermostat to 20Â°C, otherwise set it to 18Â°C.\",\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# Print the final, user-facing response\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc9788",
   "metadata": {},
   "source": [
    "### Code Execution\n",
    "\n",
    "The Gemini API provides a code execution tool that enables the model to generate and run Python code. The model can then learn iteratively from the code execution results until it arrives at a final output. You can use code execution to build applications that benefit from code-based reasoning. For example, you can use code execution to solve equations or process text. You can also use the libraries included in the code execution environment to perform more specialized tasks.\n",
    "\n",
    "Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/code-execution\n",
    "\n",
    "- Supported libraries\n",
    "The code execution environment includes the following libraries:\n",
    "\n",
    "attrs\n",
    "chess\n",
    "contourpy\n",
    "fpdf\n",
    "geopandas\n",
    "imageio\n",
    "jinja2\n",
    "joblib\n",
    "jsonschema\n",
    "jsonschema-specifications\n",
    "lxml\n",
    "matplotlib\n",
    "mpmath\n",
    "numpy\n",
    "opencv-python\n",
    "openpyxl\n",
    "packaging\n",
    "pandas\n",
    "pillow\n",
    "protobuf\n",
    "pylatex\n",
    "pyparsing\n",
    "PyPDF2\n",
    "python-dateutil\n",
    "python-docx\n",
    "python-pptx\n",
    "reportlab\n",
    "scikit-learn\n",
    "scipy\n",
    "seaborn\n",
    "six\n",
    "striprtf\n",
    "sympy\n",
    "tabulate\n",
    "tensorflow\n",
    "toolz\n",
    "xlrd\n",
    "\n",
    "###### You can't install your own libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd7eace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Please share your math question with me. I'll do my best to help you solve it.\n",
      "Of course. Here is the plan to find the sum of the first 50 prime numbers:\n",
      "\n",
      "1.  **Define a Prime Number:** A prime number is a natural number greater than 1 that has no positive divisors other than 1 and itself.\n",
      "2.  **Generate Primes:** I will write a script to find prime numbers sequentially, starting from 2.\n",
      "3.  **Collect 50 Primes:** The script will continue generating primes until it has found exactly 50 of them.\n",
      "4.  **Calculate the Sum:** Once the list of the first 50 prime numbers is complete, I will calculate their sum.\n",
      "5.  **Present Findings:** I will display the list of the 50 prime numbers found and their final sum.\n",
      "\n",
      "Here is the Python code to perform the calculation.\n",
      "\n",
      "\n",
      "def is_prime(n):\n",
      "    \"\"\"Checks if a number is prime.\"\"\"\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n <= 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    i = 5\n",
      "    while i * i <= n:\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "        i += 6\n",
      "    return True\n",
      "\n",
      "def get_first_n_primes(n):\n",
      "    \"\"\"Generates a list of the first n prime numbers.\"\"\"\n",
      "    primes = []\n",
      "    num = 2\n",
      "    while len(primes) < n:\n",
      "        if is_prime(num):\n",
      "            primes.append(num)\n",
      "        num += 1\n",
      "    return primes\n",
      "\n",
      "# 1. Generate the first 50 prime numbers\n",
      "first_50_primes = get_first_n_primes(50)\n",
      "\n",
      "# 2. Calculate their sum\n",
      "sum_of_primes = sum(first_50_primes)\n",
      "\n",
      "# 3. Present the findings\n",
      "print(f\"The first 50 prime numbers are:\\n{first_50_primes}\")\n",
      "print(f\"\\nNumber of primes found: {len(first_50_primes)}\")\n",
      "print(f\"\\nThe sum of the first 50 prime numbers is: {sum_of_primes}\")\n",
      "\n",
      "The first 50 prime numbers are:\n",
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229]\n",
      "\n",
      "Number of primes found: 50\n",
      "\n",
      "The sum of the first 50 prime numbers is: 5117\n",
      "\n",
      "### Findings\n",
      "\n",
      "Based on the calculation, here are the results:\n",
      "\n",
      "*   **The First 50 Prime Numbers:** The calculation successfully identified the first 50 prime numbers, starting with 2 and ending with 229.\n",
      "*   **The Sum:** The sum of these 50 prime numbers is **5,117**.\n"
     ]
    }
   ],
   "source": [
    "chat = client.chats.create(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat.send_message(\"I have a math question for you.\")\n",
    "print(response.text)\n",
    "\n",
    "response = chat.send_message(\n",
    "    \"What is the sum of the first 50 prime numbers? \"\n",
    "    \"Generate and run code for the calculation, and make sure you get all 50.\"\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.text is not None:\n",
    "        print(part.text)\n",
    "    if part.executable_code is not None:\n",
    "        print(part.executable_code.code)\n",
    "    if part.code_execution_result is not None:\n",
    "        print(part.code_execution_result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e876b3",
   "metadata": {},
   "source": [
    "# Using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84243af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's great to connect with you.\n",
      "\n",
      "Your message has successfully traveled from your LangChain application, through your proxy, and has reached me here on Google's Vertex AI.\n",
      "\n",
      "That's a cool setup! What can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "\n",
    "    # âœ… force Vertex AI backend (no API key required)\n",
    "    vertexai=True,\n",
    "\n",
    "    project=\"cloud-ai-police\",\n",
    "    location=\"us-central1\",\n",
    "\n",
    "    # âœ… route via your proxy\n",
    "    client_options=\"http://localhost:8080/gemini\",\n",
    "\n",
    "    temperature=0.2,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "resp = llm.invoke([HumanMessage(content=\"Hello via LangChain + proxy + Vertex!\")])\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b73970",
   "metadata": {},
   "source": [
    "# Using REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77005602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'candidates': [{'content': {'role': 'model', 'parts': [{'text': 'Hello! Message received loud and clear from your proxy.\\n\\nHow can I help you today?'}]}, 'finishReason': 'STOP', 'avgLogprobs': -12.031432302374588}], 'usageMetadata': {'promptTokenCount': 3, 'candidatesTokenCount': 19, 'totalTokenCount': 1190, 'trafficType': 'ON_DEMAND', 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 3}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 19}], 'thoughtsTokenCount': 1168}, 'modelVersion': 'gemini-2.5-pro', 'createTime': '2026-01-23T06:01:02.533564Z', 'responseId': 'ng5zabzIIO6XmecPzd6EoQ0'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/gemini/v1beta1/projects/cloud-ai-police/locations/us-central1/publishers/google/models/gemini-2.5-pro:generateContent\"\n",
    "\n",
    "body = {\n",
    "  \"contents\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        { \"text\": \"hello from proxy\" }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "res = requests.post(url, json=body)\n",
    "print(res.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa200e59",
   "metadata": {},
   "source": [
    "# Using REST b64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4a277ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body_b64': 'ewogICJjYW5kaWRhdGVzIjogWwogICAgewogICAgICAiY29udGVudCI6IHsKICAgICAgICAicm9sZSI6ICJtb2RlbCIsCiAgICAgICAgInBhcnRzIjogWwogICAgICAgICAgewogICAgICAgICAgICAidGV4dCI6ICJIZWxsbyEgTWVzc2FnZSByZWNlaXZlZCBsb3VkIGFuZCBjbGVhci4gSXQgc2VlbXMgeW91ciBwcm94eSBpcyB3b3JraW5nIHBlcmZlY3RseS5cblxuSG93IGNhbiBJIGhlbHAgeW91IHRvZGF5PyIKICAgICAgICAgIH0KICAgICAgICBdCiAgICAgIH0sCiAgICAgICJmaW5pc2hSZWFzb24iOiAiU1RPUCIsCiAgICAgICJhdmdMb2dwcm9icyI6IC04LjY4MzI0NTk3Njc2NTk1MTEKICAgIH0KICBdLAogICJ1c2FnZU1ldGFkYXRhIjogewogICAgInByb21wdFRva2VuQ291bnQiOiAzLAogICAgImNhbmRpZGF0ZXNUb2tlbkNvdW50IjogMjQsCiAgICAidG90YWxUb2tlbkNvdW50IjogMTAxOSwKICAgICJ0cmFmZmljVHlwZSI6ICJPTl9ERU1BTkQiLAogICAgInByb21wdFRva2Vuc0RldGFpbHMiOiBbCiAgICAgIHsKICAgICAgICAibW9kYWxpdHkiOiAiVEVYVCIsCiAgICAgICAgInRva2VuQ291bnQiOiAzCiAgICAgIH0KICAgIF0sCiAgICAiY2FuZGlkYXRlc1Rva2Vuc0RldGFpbHMiOiBbCiAgICAgIHsKICAgICAgICAibW9kYWxpdHkiOiAiVEVYVCIsCiAgICAgICAgInRva2VuQ291bnQiOiAyNAogICAgICB9CiAgICBdLAogICAgInRob3VnaHRzVG9rZW5Db3VudCI6IDk5MgogIH0sCiAgIm1vZGVsVmVyc2lvbiI6ICJnZW1pbmktMi41LXBybyIsCiAgImNyZWF0ZVRpbWUiOiAiMjAyNi0wMS0yM1QwNjozOTo1MS4xMDI1OTFaIiwKICAicmVzcG9uc2VJZCI6ICJ0eGR6YWItaEJwN2YtTzRQZ2NxR2tBMCIKfQo=', 'headers': {'Content-Encoding': 'gzip', 'Content-Type': 'application/json; charset=UTF-8'}, 'status_code': 200}\n"
     ]
    }
   ],
   "source": [
    "# Base64 encode the the body\n",
    "import base64,json\n",
    "\n",
    "encoded_body = base64.b64encode(json.dumps(body).encode()).decode()\n",
    "\n",
    "url = \"http://localhost:8080/geminib64/v1beta1/projects/cloud-ai-police/locations/us-central1/publishers/google/models/gemini-2.5-pro:generateContent\"\n",
    "\n",
    "res = requests.post(url, headers={\"Content-Type\": \"text/plain\"},data=encoded_body)\n",
    "\n",
    "print(res.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c1fe0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout = res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "35b60cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "def decode_b64_body(body_b64: str) -> bytes:\n",
    "    return base64.b64decode(body_b64.encode(\"utf-8\"), validate=True)\n",
    "\n",
    "def extract_json_from_stdout(stdout: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Pods may print extra logs. We try:\n",
    "      - parse whole stdout as JSON\n",
    "      - else parse last JSON object line\n",
    "    \"\"\"\n",
    "    if not stdout:\n",
    "        raise RuntimeError(\"Empty stdout from pod exec\")\n",
    "\n",
    "    # 1) direct parse\n",
    "    try:\n",
    "        return json.loads(stdout)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) attempt last line json\n",
    "    lines = [ln.strip() for ln in stdout.splitlines() if ln.strip()]\n",
    "    for ln in reversed(lines):\n",
    "        try:\n",
    "            return json.loads(ln)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3) fail with context\n",
    "    raise RuntimeError(f\"Could not parse JSON from pod stdout. Tail:\\n{stdout[-800:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4dbd8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extract_json_from_stdout(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd479333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body_b64': 'ewogICJjYW5kaWRhdGVzIjogWwogICAgewogICAgICAiY29udGVudCI6IHsKICAgICAgICAicm9sZSI6ICJtb2RlbCIsCiAgICAgICAgInBhcnRzIjogWwogICAgICAgICAgewogICAgICAgICAgICAidGV4dCI6ICJIZWxsbyEgTWVzc2FnZSByZWNlaXZlZCBsb3VkIGFuZCBjbGVhci4gSXQgc2VlbXMgeW91ciBwcm94eSBpcyB3b3JraW5nIHBlcmZlY3RseS5cblxuSG93IGNhbiBJIGhlbHAgeW91IHRvZGF5PyIKICAgICAgICAgIH0KICAgICAgICBdCiAgICAgIH0sCiAgICAgICJmaW5pc2hSZWFzb24iOiAiU1RPUCIsCiAgICAgICJhdmdMb2dwcm9icyI6IC04LjY4MzI0NTk3Njc2NTk1MTEKICAgIH0KICBdLAogICJ1c2FnZU1ldGFkYXRhIjogewogICAgInByb21wdFRva2VuQ291bnQiOiAzLAogICAgImNhbmRpZGF0ZXNUb2tlbkNvdW50IjogMjQsCiAgICAidG90YWxUb2tlbkNvdW50IjogMTAxOSwKICAgICJ0cmFmZmljVHlwZSI6ICJPTl9ERU1BTkQiLAogICAgInByb21wdFRva2Vuc0RldGFpbHMiOiBbCiAgICAgIHsKICAgICAgICAibW9kYWxpdHkiOiAiVEVYVCIsCiAgICAgICAgInRva2VuQ291bnQiOiAzCiAgICAgIH0KICAgIF0sCiAgICAiY2FuZGlkYXRlc1Rva2Vuc0RldGFpbHMiOiBbCiAgICAgIHsKICAgICAgICAibW9kYWxpdHkiOiAiVEVYVCIsCiAgICAgICAgInRva2VuQ291bnQiOiAyNAogICAgICB9CiAgICBdLAogICAgInRob3VnaHRzVG9rZW5Db3VudCI6IDk5MgogIH0sCiAgIm1vZGVsVmVyc2lvbiI6ICJnZW1pbmktMi41LXBybyIsCiAgImNyZWF0ZVRpbWUiOiAiMjAyNi0wMS0yM1QwNjozOTo1MS4xMDI1OTFaIiwKICAicmVzcG9uc2VJZCI6ICJ0eGR6YWItaEJwN2YtTzRQZ2NxR2tBMCIKfQo=',\n",
       " 'headers': {'Content-Encoding': 'gzip',\n",
       "  'Content-Type': 'application/json; charset=UTF-8'},\n",
       " 'status_code': 200}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4d830bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code = int(result.get(\"status_code\", 502))\n",
    "pod_headers = result.get(\"headers\") or {}\n",
    "body_b64_out = result.get(\"body_b64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a13b4d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body: {\n",
      "  \"candidates\": [\n",
      "    {\n",
      "      \"content\": {\n",
      "        \"role\": \"model\",\n",
      "        \"parts\": [\n",
      "          {\n",
      "            \"text\": \"Hello! Message received loud and clear. It seems your proxy is working perfectly.\\n\\nHow can I help you today?\"\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"finishReason\": \"STOP\",\n",
      "      \"avgLogprobs\": -8.6832459767659511\n",
      "    }\n",
      "  ],\n",
      "  \"usageMetadata\": {\n",
      "    \"promptTokenCount\": 3,\n",
      "    \"candidatesTokenCount\": 24,\n",
      "    \"totalTokenCount\": 1019,\n",
      "    \"trafficType\": \"ON_DEMAND\",\n",
      "    \"promptTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 3\n",
      "      }\n",
      "    ],\n",
      "    \"candidatesTokensDetails\": [\n",
      "      {\n",
      "        \"modality\": \"TEXT\",\n",
      "        \"tokenCount\": 24\n",
      "      }\n",
      "    ],\n",
      "    \"thoughtsTokenCount\": 992\n",
      "  },\n",
      "  \"modelVersion\": \"gemini-2.5-pro\",\n",
      "  \"createTime\": \"2026-01-23T06:39:51.102591Z\",\n",
      "  \"responseId\": \"txdzab-hBp7f-O4PgcqGkA0\"\n",
      "}\n",
      "\n",
      "Status Code: 200\n"
     ]
    }
   ],
   "source": [
    "if body_b64_out:\n",
    "    body_bytes = decode_b64_body(body_b64_out)\n",
    "    body_str = body_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    print(\"Body:\", body_str)\n",
    "    print(\"Status Code:\", status_code)\n",
    "else:\n",
    "    print(\"No body received from pod.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0c262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa80b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htll-ir-framework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
